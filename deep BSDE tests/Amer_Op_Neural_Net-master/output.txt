 Number of CPUs:  4
 Simulation size:  80000

 AmericanOption, initialization, phase 1 : 
   0 (0.00, 0.03), 1 (0.01, 0.02), 2 (0.01, 0.02), 3 (0.01, 0.02), 4 (0.01, 0.03), 5 (0.01, 0.02), 6 (0.01, 0.02), 7 (0.01, 0.02), 8 (0.01, 0.02), 9 (0.01, 0.02), 10 (0.01, 0.03), 11 (0.01, 0.02), 12 (0.01, 0.02), 13 (0.01, 0.02), 14 (0.01, 0.02), 15 (0.01, 0.02), 16 (0.01, 0.03), 17 (0.01, 0.02), 18 (0.01, 0.02), 19 (0.01, 0.02), 20 (0.01, 0.06), 21 (0.01, 0.06), 22 (0.01, 0.02), 23 (0.03, 0.04), 24 (0.01, 0.05), 25 (0.01, 0.02), 26 (0.01, 0.02), 27 (0.01, 0.03), 28 (0.01, 0.03), 29 (0.01, 0.02), 30 (0.01, 0.02), 
 evolution_state, time:  0.85

 AmericanOption, initialization, phase 2 : 
   0 (0.02), 1 (0.02), 2 (0.01), 3 (0.02), 4 (0.02), 5 (0.02), 6 (0.02), 7 (0.02), 8 (0.02), 9 (0.02), 10 (0.02), 11 (0.02), 12 (0.02), 13 (0.03), 14 (0.02), 15 (0.02), 16 (0.02), 17 (0.02), 18 (0.02), 19 (0.02), 20 (0.05), 21 (0.03), 22 (0.04), 23 (0.06), 24 (0.04), 25 (0.02), 26 (0.02), 27 (0.02), 28 (0.02), 29 (0.02), 30 (0.02), 
 initialize_results, time:  0.74


************** Main Script: Neural Network **************


 Trainable variables in the main neural network  AmerOp : 
    <tf.Variable 'AmerOp/layer0/batchX/beta:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer0/batchX/gamma:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer0/batchZ/beta:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer0/batchZ/gamma:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer1/WX:0' shape=(4, 1, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer1/WZ:0' shape=(4, 2, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer1/batchZ/beta:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer1/batchZ/gamma:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer2/WZ:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer2/batchZ/beta:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer2/batchZ/gamma:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer3/WZ:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer3/batchZ/beta:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer3/batchZ/gamma:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer4/WZ:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer4/batchZ/beta:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer4/batchZ/gamma:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer5/WZ:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer5/batchZ/beta:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer5/batchZ/gamma:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer6/WZ:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer6/batchZ/beta:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer6/batchZ/gamma:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/alpha:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer7/WZ:0' shape=(4, 7, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer7/b:0' shape=(1, 4, 1) dtype=float64_ref>

 Global variables in the main neural network  AmerOp : 
    <tf.Variable 'AmerOp/step:0' shape=() dtype=int32_ref>
    <tf.Variable 'AmerOp/init_rate:0' shape=() dtype=float64_ref>
    <tf.Variable 'AmerOp/decay_rate:0' shape=() dtype=float64_ref>
    <tf.Variable 'AmerOp/n_relaxstep:0' shape=() dtype=int32_ref>
    <tf.Variable 'AmerOp/n_decaystep:0' shape=() dtype=int32_ref>
    <tf.Variable 'AmerOp/layer0/batchX/mv_mean:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer0/batchX/mv_var:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer0/batchX/beta:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer0/batchX/gamma:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer0/batchZ/mv_mean:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer0/batchZ/mv_var:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer0/batchZ/beta:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer0/batchZ/gamma:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer1/WX:0' shape=(4, 1, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer1/WZ:0' shape=(4, 2, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer1/batchZ/mv_mean:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer1/batchZ/mv_var:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer1/batchZ/beta:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer1/batchZ/gamma:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer2/WZ:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer2/batchZ/mv_mean:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer2/batchZ/mv_var:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer2/batchZ/beta:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer2/batchZ/gamma:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer3/WZ:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer3/batchZ/mv_mean:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer3/batchZ/mv_var:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer3/batchZ/beta:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer3/batchZ/gamma:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer4/WZ:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer4/batchZ/mv_mean:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer4/batchZ/mv_var:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer4/batchZ/beta:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer4/batchZ/gamma:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer5/WZ:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer5/batchZ/mv_mean:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer5/batchZ/mv_var:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer5/batchZ/beta:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer5/batchZ/gamma:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer6/WZ:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer6/batchZ/mv_mean:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer6/batchZ/mv_var:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer6/batchZ/beta:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer6/batchZ/gamma:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/delta:0' shape=() dtype=float64_ref>
    <tf.Variable 'AmerOp/alpha:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer7/WZ:0' shape=(4, 7, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/layer7/b:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchX/beta/COCOB:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchX/beta/COCOB_1:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchX/beta/COCOB_2:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchX/beta/COCOB_3:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchX/beta/COCOB_4:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchX/gamma/COCOB:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchX/gamma/COCOB_1:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchX/gamma/COCOB_2:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchX/gamma/COCOB_3:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchX/gamma/COCOB_4:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchZ/beta/COCOB:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchZ/beta/COCOB_1:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchZ/beta/COCOB_2:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchZ/beta/COCOB_3:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchZ/beta/COCOB_4:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchZ/gamma/COCOB:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchZ/gamma/COCOB_1:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchZ/gamma/COCOB_2:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchZ/gamma/COCOB_3:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer0/batchZ/gamma/COCOB_4:0' shape=(1, 4, 2) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/WX/COCOB:0' shape=(4, 1, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/WX/COCOB_1:0' shape=(4, 1, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/WX/COCOB_2:0' shape=(4, 1, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/WX/COCOB_3:0' shape=(4, 1, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/WX/COCOB_4:0' shape=(4, 1, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/WZ/COCOB:0' shape=(4, 2, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/WZ/COCOB_1:0' shape=(4, 2, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/WZ/COCOB_2:0' shape=(4, 2, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/WZ/COCOB_3:0' shape=(4, 2, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/WZ/COCOB_4:0' shape=(4, 2, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/batchZ/beta/COCOB:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/batchZ/beta/COCOB_1:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/batchZ/beta/COCOB_2:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/batchZ/beta/COCOB_3:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/batchZ/beta/COCOB_4:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/batchZ/gamma/COCOB:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/batchZ/gamma/COCOB_1:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/batchZ/gamma/COCOB_2:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/batchZ/gamma/COCOB_3:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer1/batchZ/gamma/COCOB_4:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/WZ/COCOB:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/WZ/COCOB_1:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/WZ/COCOB_2:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/WZ/COCOB_3:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/WZ/COCOB_4:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/batchZ/beta/COCOB:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/batchZ/beta/COCOB_1:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/batchZ/beta/COCOB_2:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/batchZ/beta/COCOB_3:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/batchZ/beta/COCOB_4:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/batchZ/gamma/COCOB:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/batchZ/gamma/COCOB_1:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/batchZ/gamma/COCOB_2:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/batchZ/gamma/COCOB_3:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer2/batchZ/gamma/COCOB_4:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/WZ/COCOB:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/WZ/COCOB_1:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/WZ/COCOB_2:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/WZ/COCOB_3:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/WZ/COCOB_4:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/batchZ/beta/COCOB:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/batchZ/beta/COCOB_1:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/batchZ/beta/COCOB_2:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/batchZ/beta/COCOB_3:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/batchZ/beta/COCOB_4:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/batchZ/gamma/COCOB:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/batchZ/gamma/COCOB_1:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/batchZ/gamma/COCOB_2:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/batchZ/gamma/COCOB_3:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer3/batchZ/gamma/COCOB_4:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/WZ/COCOB:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/WZ/COCOB_1:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/WZ/COCOB_2:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/WZ/COCOB_3:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/WZ/COCOB_4:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/batchZ/beta/COCOB:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/batchZ/beta/COCOB_1:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/batchZ/beta/COCOB_2:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/batchZ/beta/COCOB_3:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/batchZ/beta/COCOB_4:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/batchZ/gamma/COCOB:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/batchZ/gamma/COCOB_1:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/batchZ/gamma/COCOB_2:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/batchZ/gamma/COCOB_3:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer4/batchZ/gamma/COCOB_4:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/WZ/COCOB:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/WZ/COCOB_1:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/WZ/COCOB_2:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/WZ/COCOB_3:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/WZ/COCOB_4:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/batchZ/beta/COCOB:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/batchZ/beta/COCOB_1:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/batchZ/beta/COCOB_2:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/batchZ/beta/COCOB_3:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/batchZ/beta/COCOB_4:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/batchZ/gamma/COCOB:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/batchZ/gamma/COCOB_1:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/batchZ/gamma/COCOB_2:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/batchZ/gamma/COCOB_3:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer5/batchZ/gamma/COCOB_4:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/WZ/COCOB:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/WZ/COCOB_1:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/WZ/COCOB_2:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/WZ/COCOB_3:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/WZ/COCOB_4:0' shape=(4, 7, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/batchZ/beta/COCOB:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/batchZ/beta/COCOB_1:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/batchZ/beta/COCOB_2:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/batchZ/beta/COCOB_3:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/batchZ/beta/COCOB_4:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/batchZ/gamma/COCOB:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/batchZ/gamma/COCOB_1:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/batchZ/gamma/COCOB_2:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/batchZ/gamma/COCOB_3:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer6/batchZ/gamma/COCOB_4:0' shape=(1, 4, 7) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/alpha/COCOB:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/alpha/COCOB_1:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/alpha/COCOB_2:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/alpha/COCOB_3:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/alpha/COCOB_4:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer7/WZ/COCOB:0' shape=(4, 7, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer7/WZ/COCOB_1:0' shape=(4, 7, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer7/WZ/COCOB_2:0' shape=(4, 7, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer7/WZ/COCOB_3:0' shape=(4, 7, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer7/WZ/COCOB_4:0' shape=(4, 7, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer7/b/COCOB:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer7/b/COCOB_1:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer7/b/COCOB_2:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer7/b/COCOB_3:0' shape=(1, 4, 1) dtype=float64_ref>
    <tf.Variable 'AmerOp/AmerOp/layer7/b/COCOB_4:0' shape=(1, 4, 1) dtype=float64_ref>


************** n =  29  **************


 **** n =  29 , Pre-processing 

 **** n =  29 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  0.000000 ,  gradY :  -0.595949 ,  regloss :  11872.983186
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.911122 ,  b :  0.020000 ,  gradY :  -0.539615 ,  regloss :  22637.663285
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.830064 ,  b :  0.023698 ,  gradY :  -0.619055 ,  regloss :  11097.800762
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.756139 ,  b :  0.019820 ,  gradY :  -0.594230 ,  regloss :  10104.064864
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.688718 ,  b :  0.033044 ,  gradY :  -0.604005 ,  regloss :  11263.972596
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.627230 ,  b :  0.039325 ,  gradY :  -0.557915 ,  regloss :  14137.096061
 step :     7 ,  learning rate:  0.009120 ,  batch learning rate:  0.571151 ,  b :  0.050453 ,  gradY :  -0.589711 ,  regloss :  13547.569145
 step :     8 ,  learning rate:  0.008318 ,  batch learning rate:  0.520008 ,  b :  0.061065 ,  gradY :  -0.562196 ,  regloss :  14603.993399
 step :     9 ,  learning rate:  0.007586 ,  batch learning rate:  0.473364 ,  b :  0.073062 ,  gradY :  -0.608269 ,  regloss :  15192.948033
 step :    10 ,  learning rate:  0.006918 ,  batch learning rate:  0.430824 ,  b :  0.084190 ,  gradY :  -0.583641 ,  regloss :  10749.623738
 step :    11 ,  learning rate:  0.006310 ,  batch learning rate:  0.392027 ,  b :  0.094464 ,  gradY :  -0.601374 ,  regloss :  17460.131234
 step :    12 ,  learning rate:  0.005754 ,  batch learning rate:  0.356645 ,  b :  0.111749 ,  gradY :  -0.570280 ,  regloss :  11911.794231
 step :    13 ,  learning rate:  0.005248 ,  batch learning rate:  0.324375 ,  b :  0.130490 ,  gradY :  -0.577247 ,  regloss :  12631.307848
 step :    14 ,  learning rate:  0.004786 ,  batch learning rate:  0.294945 ,  b :  0.147308 ,  gradY :  -0.574858 ,  regloss :  13126.590307
 step :    15 ,  learning rate:  0.004365 ,  batch learning rate:  0.268104 ,  b :  0.163285 ,  gradY :  -0.598201 ,  regloss :  13991.316434
 step :    16 ,  learning rate:  0.003981 ,  batch learning rate:  0.243625 ,  b :  0.190393 ,  gradY :  -0.591977 ,  regloss :  16921.777803
 step :    17 ,  learning rate:  0.003631 ,  batch learning rate:  0.221300 ,  b :  0.216996 ,  gradY :  -0.567072 ,  regloss :  7424.090520
 step :    18 ,  learning rate:  0.003311 ,  batch learning rate:  0.200939 ,  b :  0.239943 ,  gradY :  -0.574983 ,  regloss :  8096.824082
 step :    19 ,  learning rate:  0.003020 ,  batch learning rate:  0.182370 ,  b :  0.253781 ,  gradY :  -0.608036 ,  regloss :  18220.615688
 step :    20 ,  learning rate:  0.002754 ,  batch learning rate:  0.165434 ,  b :  0.296860 ,  gradY :  -0.583392 ,  regloss :  16353.280085
 step :    21 ,  learning rate:  0.002512 ,  batch learning rate:  0.149989 ,  b :  0.341517 ,  gradY :  -0.569416 ,  regloss :  14564.042451
 step :    41 ,  learning rate:  0.000398 ,  batch learning rate:  0.015272 ,  b :  1.258095 ,  gradY :  -0.601656 ,  regloss :  9428.194940
 step :    61 ,  learning rate:  0.000100 ,  batch learning rate:  -0.000000 ,  b :  3.727339 ,  gradY :  -0.587597 ,  regloss :  17113.235618
 step :    81 ,  learning rate:  0.000100 ,  batch learning rate:  -0.000000 ,  b :  4.602823 ,  gradY :  -0.568856 ,  regloss :  24670.371711
 step :   100 ,  learning rate:  0.000100 ,  batch learning rate:  -0.000000 ,  b :  8.070166 ,  gradY :  -0.564487 ,  regloss :  15439.367101

   time:  5.47

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  7.909644 ,  gradY :  -0.554280 ,  regloss :  13625.300858
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  8.115379 ,  gradY :  -0.628693 ,  regloss :  14388.756354
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  7.957082 ,  gradY :  -0.588153 ,  regloss :  15209.218880
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  8.198961 ,  gradY :  -0.594594 ,  regloss :  7350.456547
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  8.020859 ,  gradY :  -0.551141 ,  regloss :  9258.073353
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  7.742923 ,  gradY :  -0.586531 ,  regloss :  18290.394604
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  8.252774 ,  gradY :  -0.598388 ,  regloss :  14176.260926
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  8.371632 ,  gradY :  -0.564570 ,  regloss :  9919.842483
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  8.107374 ,  gradY :  -0.601785 ,  regloss :  8497.202524
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  7.653744 ,  gradY :  -0.584765 ,  regloss :  21485.612355
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  8.333479 ,  gradY :  -0.580148 ,  regloss :  10173.394945
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  8.011341 ,  gradY :  -0.571683 ,  regloss :  8215.032952
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  7.796580 ,  gradY :  -0.587745 ,  regloss :  9826.772086
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  7.745161 ,  gradY :  -0.592330 ,  regloss :  8883.214322
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  7.686986 ,  gradY :  -0.606517 ,  regloss :  9712.238008
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  7.567114 ,  gradY :  -0.581095 ,  regloss :  15303.874742
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  8.020533 ,  gradY :  -0.549381 ,  regloss :  10889.943498
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  7.938743 ,  gradY :  -0.600759 ,  regloss :  11725.715299
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  8.083425 ,  gradY :  -0.584775 ,  regloss :  10344.602142
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  8.298578 ,  gradY :  -0.572239 ,  regloss :  13335.905095
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  8.735741 ,  gradY :  -0.602090 ,  regloss :  7451.845813
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  10.726942 ,  gradY :  -0.568505 ,  regloss :  7491.463996
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  9.277349 ,  gradY :  -0.563600 ,  regloss :  13708.783489
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  7.473808 ,  gradY :  -0.600344 ,  regloss :  11863.966052
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  7.857996 ,  gradY :  -0.584567 ,  regloss :  5849.883630

   time:  1.52

 Ensemble Average :  29
 time: 1.21

 **** n =  29 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  32314

 Evaluate Y and gradY, timestep-wise : 


************** n =  28  **************


 **** n =  28 , Pre-processing 

 **** n =  28 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  7.658754 ,  gradY :  -0.585660 ,  regloss :  19586.149429
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  4.915182 ,  gradY :  -0.585228 ,  regloss :  23659.911067
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.383449 ,  gradY :  -0.571978 ,  regloss :  23853.245581
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  6.101191 ,  gradY :  -0.594843 ,  regloss :  22639.735022
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  6.276339 ,  gradY :  -0.597834 ,  regloss :  28212.240086
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  6.651742 ,  gradY :  -0.600127 ,  regloss :  28739.512561
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  7.147744 ,  gradY :  -0.581940 ,  regloss :  28709.194260
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  7.026890 ,  gradY :  -0.596325 ,  regloss :  26658.637342
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  6.786105 ,  gradY :  -0.610101 ,  regloss :  27690.678392
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  7.793815 ,  gradY :  -0.579083 ,  regloss :  24362.936978
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  7.222419 ,  gradY :  -0.591378 ,  regloss :  24797.028086
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  7.449027 ,  gradY :  -0.602870 ,  regloss :  33810.549798
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  8.496373 ,  gradY :  -0.568872 ,  regloss :  24423.239360
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  4.976700 ,  gradY :  -0.565295 ,  regloss :  22916.055463
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  4.360293 ,  gradY :  -0.607460 ,  regloss :  23647.152214
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  4.972187 ,  gradY :  -0.609889 ,  regloss :  29126.282587
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  4.907166 ,  gradY :  -0.598441 ,  regloss :  30464.024748
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  5.815933 ,  gradY :  -0.599666 ,  regloss :  26421.372384
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  5.982473 ,  gradY :  -0.587585 ,  regloss :  30046.618866
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  6.469311 ,  gradY :  -0.572813 ,  regloss :  26065.306352
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  6.688364 ,  gradY :  -0.601358 ,  regloss :  28470.426491
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.536230 ,  gradY :  -0.566368 ,  regloss :  24432.853687
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  8.670933 ,  gradY :  -0.553653 ,  regloss :  22208.594590
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  8.730830 ,  gradY :  -0.580088 ,  regloss :  30374.756043
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  9.732136 ,  gradY :  -0.555853 ,  regloss :  31426.796472

   time:  1.57

 Ensemble Average :  1
 time: 0.81

 Ensemble Average :  2
 time: 0.81

 Ensemble Average :  3
 time: 0.83

 Ensemble Average :  4
 time: 0.81

 Ensemble Average :  5
 time: 1.00

 Ensemble Average :  6
 time: 0.93

 Ensemble Average :  7
 time: 0.81

 Ensemble Average :  8
 time: 0.80

 Ensemble Average :  9
 time: 0.83

 Ensemble Average :  10
 time: 0.84

 Ensemble Average :  11
 time: 0.84

 Ensemble Average :  12
 time: 0.82

 Ensemble Average :  13
 time: 0.82

 Ensemble Average :  14
 time: 0.82

 Ensemble Average :  15
 time: 0.81

 Ensemble Average :  16
 time: 0.81

 Ensemble Average :  17
 time: 0.94

 Ensemble Average :  18
 time: 0.88

 Ensemble Average :  19
 time: 0.99

 Ensemble Average :  20
 time: 0.88

 Ensemble Average :  21
 time: 0.81

 Ensemble Average :  22
 time: 0.83

 Ensemble Average :  23
 time: 0.86

 Ensemble Average :  24
 time: 0.84

 Ensemble Average :  25
 time: 0.81

 Ensemble Average :  26
 time: 0.81

 Ensemble Average :  27
 time: 0.80

 Ensemble Average :  28
 time: 0.82

 **** n =  28 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  17094

 Evaluate Y and gradY, timestep-wise : 


************** n =  27  **************


 **** n =  27 , Pre-processing 

 **** n =  27 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  10.697357 ,  gradY :  -0.588877 ,  regloss :  34083.704327
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  10.735481 ,  gradY :  -0.588496 ,  regloss :  35674.024023
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  10.483387 ,  gradY :  -0.559461 ,  regloss :  32376.473893
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  10.437839 ,  gradY :  -0.555098 ,  regloss :  33087.437858
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  9.905902 ,  gradY :  -0.548401 ,  regloss :  36136.128204
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  9.746699 ,  gradY :  -0.585921 ,  regloss :  31536.112291
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  9.622800 ,  gradY :  -0.574143 ,  regloss :  32349.007084
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  9.663414 ,  gradY :  -0.607838 ,  regloss :  36127.513841
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  9.690404 ,  gradY :  -0.583244 ,  regloss :  31671.748383
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  9.266899 ,  gradY :  -0.594762 ,  regloss :  36462.986726
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  9.018876 ,  gradY :  -0.571773 ,  regloss :  37800.888286
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  9.156557 ,  gradY :  -0.588781 ,  regloss :  34839.288409
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  8.958232 ,  gradY :  -0.557504 ,  regloss :  30232.056220
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  8.903620 ,  gradY :  -0.552992 ,  regloss :  34658.252456
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  8.785013 ,  gradY :  -0.588965 ,  regloss :  33808.439915
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  8.388676 ,  gradY :  -0.577138 ,  regloss :  29648.428069
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  8.481442 ,  gradY :  -0.580856 ,  regloss :  39547.792354
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  8.580938 ,  gradY :  -0.580080 ,  regloss :  35606.208441
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  8.061325 ,  gradY :  -0.592706 ,  regloss :  33277.239999
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  8.137216 ,  gradY :  -0.525376 ,  regloss :  35467.458118
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  8.112494 ,  gradY :  -0.578664 ,  regloss :  34825.021647
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  7.687246 ,  gradY :  -0.591466 ,  regloss :  30405.159976
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  7.797854 ,  gradY :  -0.577301 ,  regloss :  34284.762781
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  7.111363 ,  gradY :  -0.605038 ,  regloss :  36209.093466
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  7.182265 ,  gradY :  -0.573434 ,  regloss :  29930.342971

   time:  1.53

 Ensemble Average :  27
 time: 0.82

 **** n =  27 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  23592

 Evaluate Y and gradY, timestep-wise : 


************** n =  26  **************


 **** n =  26 , Pre-processing 

 **** n =  26 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  7.219895 ,  gradY :  -0.587734 ,  regloss :  31946.005332
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  7.483425 ,  gradY :  -0.561342 ,  regloss :  26988.028941
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  7.060623 ,  gradY :  -0.571448 ,  regloss :  30140.250767
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  7.629471 ,  gradY :  -0.583864 ,  regloss :  28101.507366
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  7.674503 ,  gradY :  -0.606128 ,  regloss :  26287.612397
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  7.485373 ,  gradY :  -0.591659 ,  regloss :  31269.242074
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  7.848046 ,  gradY :  -0.599678 ,  regloss :  31202.647664
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  8.273887 ,  gradY :  -0.609947 ,  regloss :  27043.330994
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  8.214565 ,  gradY :  -0.600637 ,  regloss :  32223.092647
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  8.915277 ,  gradY :  -0.601118 ,  regloss :  30006.525442
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  7.981349 ,  gradY :  -0.605360 ,  regloss :  30349.679845
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  8.445356 ,  gradY :  -0.587216 ,  regloss :  30033.876616
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  8.757395 ,  gradY :  -0.596696 ,  regloss :  24637.213674
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  8.425464 ,  gradY :  -0.574595 ,  regloss :  31765.151093
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  8.754968 ,  gradY :  -0.594151 ,  regloss :  38988.930796
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  8.418047 ,  gradY :  -0.569736 ,  regloss :  37969.441578
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  8.034121 ,  gradY :  -0.584413 ,  regloss :  26918.528562
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  8.386104 ,  gradY :  -0.585512 ,  regloss :  31464.342926
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  8.025257 ,  gradY :  -0.589939 ,  regloss :  29375.904565
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  8.736369 ,  gradY :  -0.608501 ,  regloss :  29156.491654
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  8.580563 ,  gradY :  -0.553047 ,  regloss :  29865.235663
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.652984 ,  gradY :  -0.580901 ,  regloss :  32385.604409
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  7.003462 ,  gradY :  -0.577191 ,  regloss :  29890.985443
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.552697 ,  gradY :  -0.590320 ,  regloss :  31586.306511
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  4.804834 ,  gradY :  -0.588600 ,  regloss :  28499.536492

   time:  1.59

 Ensemble Average :  1
 time: 1.03

 Ensemble Average :  2
 time: 1.00

 Ensemble Average :  3
 time: 0.89

 Ensemble Average :  4
 time: 1.32

 Ensemble Average :  5
 time: 1.13

 Ensemble Average :  6
 time: 1.72

 Ensemble Average :  7
 time: 1.09

 Ensemble Average :  8
 time: 0.96

 Ensemble Average :  9
 time: 0.84

 Ensemble Average :  10
 time: 0.92

 Ensemble Average :  11
 time: 0.90

 Ensemble Average :  12
 time: 0.96

 Ensemble Average :  13
 time: 0.94

 Ensemble Average :  14
 time: 0.81

 Ensemble Average :  15
 time: 0.81

 Ensemble Average :  16
 time: 0.83

 Ensemble Average :  17
 time: 0.80

 Ensemble Average :  18
 time: 0.80

 Ensemble Average :  19
 time: 0.80

 Ensemble Average :  20
 time: 0.82

 Ensemble Average :  21
 time: 0.81

 Ensemble Average :  22
 time: 0.80

 Ensemble Average :  23
 time: 0.80

 Ensemble Average :  24
 time: 0.81

 Ensemble Average :  25
 time: 0.92

 Ensemble Average :  26
 time: 0.85

 **** n =  26 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  15898

 Evaluate Y and gradY, timestep-wise : 


************** n =  25  **************


 **** n =  25 , Pre-processing 

 **** n =  25 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  4.798690 ,  gradY :  -0.557899 ,  regloss :  34446.416945
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  4.856078 ,  gradY :  -0.560569 ,  regloss :  31744.555320
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.025908 ,  gradY :  -0.579663 ,  regloss :  27261.189556
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  5.150847 ,  gradY :  -0.592150 ,  regloss :  34367.053295
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.248249 ,  gradY :  -0.591533 ,  regloss :  35857.351412
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.376664 ,  gradY :  -0.581232 ,  regloss :  32197.593699
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.263030 ,  gradY :  -0.564263 ,  regloss :  26665.257828
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.292826 ,  gradY :  -0.578305 ,  regloss :  36957.590749
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  5.378889 ,  gradY :  -0.573235 ,  regloss :  29034.734309
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.311854 ,  gradY :  -0.579598 ,  regloss :  32599.590713
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.310679 ,  gradY :  -0.588372 ,  regloss :  31634.047924
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  5.283152 ,  gradY :  -0.598613 ,  regloss :  28507.730483
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  5.120403 ,  gradY :  -0.568816 ,  regloss :  32947.151250
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  5.204208 ,  gradY :  -0.591534 ,  regloss :  31889.547201
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  5.286518 ,  gradY :  -0.562062 ,  regloss :  34327.555642
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  5.316305 ,  gradY :  -0.554457 ,  regloss :  25702.731026
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  5.322139 ,  gradY :  -0.592321 ,  regloss :  35458.177449
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  5.408761 ,  gradY :  -0.572822 ,  regloss :  34797.172687
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  5.401825 ,  gradY :  -0.584844 ,  regloss :  30936.576179
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  5.483290 ,  gradY :  -0.589043 ,  regloss :  33262.769102
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  5.418632 ,  gradY :  -0.599159 ,  regloss :  30794.839157
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  5.269048 ,  gradY :  -0.574678 ,  regloss :  30617.907860
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.395487 ,  gradY :  -0.583539 ,  regloss :  27061.962636
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.343742 ,  gradY :  -0.585244 ,  regloss :  31356.536947
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.507520 ,  gradY :  -0.558026 ,  regloss :  32361.953867

   time:  2.01

 Ensemble Average :  25
 time: 0.91

 **** n =  25 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  26071

 Evaluate Y and gradY, timestep-wise : 


************** n =  24  **************


 **** n =  24 , Pre-processing 

 **** n =  24 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  5.477002 ,  gradY :  -0.554496 ,  regloss :  26062.408549
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  5.373153 ,  gradY :  -0.556849 ,  regloss :  26565.548064
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.393406 ,  gradY :  -0.599052 ,  regloss :  20408.977958
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  5.382534 ,  gradY :  -0.594887 ,  regloss :  24560.733556
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.409298 ,  gradY :  -0.595104 ,  regloss :  23306.940960
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.430804 ,  gradY :  -0.587521 ,  regloss :  23608.245045
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.455215 ,  gradY :  -0.570119 ,  regloss :  23899.400150
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.538843 ,  gradY :  -0.590032 ,  regloss :  20937.651536
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  5.703937 ,  gradY :  -0.600343 ,  regloss :  25765.439447
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.642605 ,  gradY :  -0.586956 ,  regloss :  19348.270169
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.646894 ,  gradY :  -0.556918 ,  regloss :  20229.302205
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  5.584039 ,  gradY :  -0.570392 ,  regloss :  19275.186149
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  5.524443 ,  gradY :  -0.574784 ,  regloss :  21619.976205
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  5.645172 ,  gradY :  -0.619882 ,  regloss :  28908.648931
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  5.779556 ,  gradY :  -0.571251 ,  regloss :  20329.626739
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  5.532933 ,  gradY :  -0.573651 ,  regloss :  22102.339409
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  5.661078 ,  gradY :  -0.594996 ,  regloss :  20619.182338
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  5.512691 ,  gradY :  -0.585260 ,  regloss :  22256.521285
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  5.579484 ,  gradY :  -0.561782 ,  regloss :  22209.128634
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  5.521385 ,  gradY :  -0.595154 ,  regloss :  23601.989345
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  5.651533 ,  gradY :  -0.564878 ,  regloss :  22131.750648
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  5.721159 ,  gradY :  -0.559924 ,  regloss :  22034.345525
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.581168 ,  gradY :  -0.598273 ,  regloss :  20247.004033
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.464420 ,  gradY :  -0.601348 ,  regloss :  20540.890294
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.414442 ,  gradY :  -0.585127 ,  regloss :  23729.971405

   time:  1.67

 Ensemble Average :  1
 time: 0.87

 Ensemble Average :  2
 time: 0.86

 Ensemble Average :  3
 time: 0.87

 Ensemble Average :  4
 time: 0.88

 Ensemble Average :  5
 time: 0.87

 Ensemble Average :  6
 time: 0.86

 Ensemble Average :  7
 time: 1.00

 Ensemble Average :  8
 time: 0.88

 Ensemble Average :  9
 time: 0.88

 Ensemble Average :  10
 time: 0.88

 Ensemble Average :  11
 time: 0.88

 Ensemble Average :  12
 time: 0.85

 Ensemble Average :  13
 time: 0.87

 Ensemble Average :  14
 time: 0.87

 Ensemble Average :  15
 time: 0.86

 Ensemble Average :  16
 time: 0.89

 Ensemble Average :  17
 time: 0.87

 Ensemble Average :  18
 time: 0.88

 Ensemble Average :  19
 time: 0.87

 Ensemble Average :  20
 time: 0.87

 Ensemble Average :  21
 time: 0.86

 Ensemble Average :  22
 time: 0.88

 Ensemble Average :  23
 time: 0.86

 Ensemble Average :  24
 time: 0.88

 **** n =  24 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  26366

 Evaluate Y and gradY, timestep-wise : 


************** n =  23  **************


 **** n =  23 , Pre-processing 

 **** n =  23 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  5.244989 ,  gradY :  -0.569242 ,  regloss :  20599.287931
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  5.318314 ,  gradY :  -0.581798 ,  regloss :  18914.652394
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.356625 ,  gradY :  -0.572975 ,  regloss :  21639.637079
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  5.498518 ,  gradY :  -0.571665 ,  regloss :  19368.609990
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.527351 ,  gradY :  -0.579090 ,  regloss :  20714.821274
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.517505 ,  gradY :  -0.573243 ,  regloss :  18713.050207
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.513836 ,  gradY :  -0.567506 ,  regloss :  19812.424315
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.593303 ,  gradY :  -0.571877 ,  regloss :  20257.998357
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  5.589523 ,  gradY :  -0.581930 ,  regloss :  19370.788942
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.600184 ,  gradY :  -0.566038 ,  regloss :  21130.310607
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.638490 ,  gradY :  -0.584038 ,  regloss :  18275.376303
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  5.562829 ,  gradY :  -0.594281 ,  regloss :  23281.271017
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  5.610427 ,  gradY :  -0.584955 ,  regloss :  18243.105127
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  5.574510 ,  gradY :  -0.609233 ,  regloss :  21818.688983
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  5.569910 ,  gradY :  -0.576895 ,  regloss :  20528.970526
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  5.527850 ,  gradY :  -0.588912 ,  regloss :  20289.440356
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  5.488430 ,  gradY :  -0.585911 ,  regloss :  22912.087635
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  5.481738 ,  gradY :  -0.581122 ,  regloss :  19405.440127
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  5.523871 ,  gradY :  -0.604213 ,  regloss :  21495.734235
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  5.554143 ,  gradY :  -0.580673 ,  regloss :  18838.190698
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  5.561082 ,  gradY :  -0.579254 ,  regloss :  20570.364448
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  5.437195 ,  gradY :  -0.564840 ,  regloss :  19701.041960
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.560062 ,  gradY :  -0.595322 ,  regloss :  18170.870283
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.703121 ,  gradY :  -0.565307 ,  regloss :  22463.717167
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.692742 ,  gradY :  -0.576484 ,  regloss :  19502.465017

   time:  1.68

 Ensemble Average :  23
 time: 0.85

 **** n =  23 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  25046

 Evaluate Y and gradY, timestep-wise : 


************** n =  22  **************


 **** n =  22 , Pre-processing 

 **** n =  22 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  5.771256 ,  gradY :  -0.579097 ,  regloss :  19781.540616
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  5.668297 ,  gradY :  -0.604875 ,  regloss :  23669.568600
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.699621 ,  gradY :  -0.567587 ,  regloss :  20120.827803
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  5.724438 ,  gradY :  -0.576049 ,  regloss :  22617.835365
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.809851 ,  gradY :  -0.595864 ,  regloss :  18935.017437
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.657314 ,  gradY :  -0.572268 ,  regloss :  18971.078871
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.723085 ,  gradY :  -0.588197 ,  regloss :  20064.911829
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.572472 ,  gradY :  -0.579472 ,  regloss :  20257.953316
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  5.530485 ,  gradY :  -0.574104 ,  regloss :  20860.640700
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.562958 ,  gradY :  -0.581761 ,  regloss :  21889.112835
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.637603 ,  gradY :  -0.585430 ,  regloss :  23542.423346
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  5.426793 ,  gradY :  -0.592652 ,  regloss :  19699.237775
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  5.415426 ,  gradY :  -0.586472 ,  regloss :  22461.869400
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  5.549720 ,  gradY :  -0.618344 ,  regloss :  24398.203591
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  5.567279 ,  gradY :  -0.591480 ,  regloss :  23980.315665
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  5.776609 ,  gradY :  -0.574739 ,  regloss :  18741.782064
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  5.779738 ,  gradY :  -0.595297 ,  regloss :  20870.928164
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  5.678737 ,  gradY :  -0.593953 ,  regloss :  19080.135687
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  5.763008 ,  gradY :  -0.596447 ,  regloss :  21935.862118
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  5.806658 ,  gradY :  -0.597794 ,  regloss :  22307.918162
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  6.085688 ,  gradY :  -0.588651 ,  regloss :  19519.571511
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  7.051491 ,  gradY :  -0.597391 ,  regloss :  21541.051505
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  7.391258 ,  gradY :  -0.581687 ,  regloss :  19686.227396
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  7.974779 ,  gradY :  -0.566834 ,  regloss :  20809.222267
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  7.680139 ,  gradY :  -0.586070 ,  regloss :  20701.688401

   time:  1.65

 Ensemble Average :  1
 time: 0.98

 Ensemble Average :  2
 time: 1.02

 Ensemble Average :  3
 time: 0.94

 Ensemble Average :  4
 time: 0.91

 Ensemble Average :  5
 time: 0.91

 Ensemble Average :  6
 time: 1.07

 Ensemble Average :  7
 time: 1.02

 Ensemble Average :  8
 time: 0.89

 Ensemble Average :  9
 time: 0.89

 Ensemble Average :  10
 time: 0.88

 Ensemble Average :  11
 time: 1.14

 Ensemble Average :  12
 time: 0.90

 Ensemble Average :  13
 time: 1.27

 Ensemble Average :  14
 time: 1.05

 Ensemble Average :  15
 time: 1.14

 Ensemble Average :  16
 time: 0.81

 Ensemble Average :  17
 time: 0.86

 Ensemble Average :  18
 time: 0.93

 Ensemble Average :  19
 time: 0.94

 Ensemble Average :  20
 time: 1.08

 Ensemble Average :  21
 time: 0.92

 Ensemble Average :  22
 time: 0.94

 **** n =  22 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  25009

 Evaluate Y and gradY, timestep-wise : 


************** n =  21  **************


 **** n =  21 , Pre-processing 

 **** n =  21 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  7.819037 ,  gradY :  -0.586588 ,  regloss :  17516.938683
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  7.758484 ,  gradY :  -0.576707 ,  regloss :  20396.628195
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  7.773813 ,  gradY :  -0.574387 ,  regloss :  20631.732171
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  7.738779 ,  gradY :  -0.564481 ,  regloss :  20138.744821
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  7.719336 ,  gradY :  -0.584492 ,  regloss :  18752.035758
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  7.673394 ,  gradY :  -0.579105 ,  regloss :  19249.486368
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  7.618056 ,  gradY :  -0.576707 ,  regloss :  19770.013481
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  7.632811 ,  gradY :  -0.599306 ,  regloss :  20549.781409
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  7.555945 ,  gradY :  -0.580796 ,  regloss :  22143.150178
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  7.583855 ,  gradY :  -0.576711 ,  regloss :  19731.982820
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  7.489432 ,  gradY :  -0.594264 ,  regloss :  17281.224359
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  7.510032 ,  gradY :  -0.567536 ,  regloss :  22217.453782
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  7.585637 ,  gradY :  -0.568002 ,  regloss :  22286.128699
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  7.549902 ,  gradY :  -0.595610 ,  regloss :  22288.716650
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  7.534960 ,  gradY :  -0.554508 ,  regloss :  16595.319854
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  7.513165 ,  gradY :  -0.586075 ,  regloss :  20837.891980
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  7.573187 ,  gradY :  -0.581358 ,  regloss :  18309.162192
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  7.569795 ,  gradY :  -0.587689 ,  regloss :  20171.481233
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  7.383718 ,  gradY :  -0.578931 ,  regloss :  20021.607077
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  7.463241 ,  gradY :  -0.562874 ,  regloss :  21168.672330
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  7.429451 ,  gradY :  -0.573326 ,  regloss :  20397.560091
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  7.273871 ,  gradY :  -0.570001 ,  regloss :  22180.351799
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  7.339220 ,  gradY :  -0.597841 ,  regloss :  19962.413028
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  7.347541 ,  gradY :  -0.584314 ,  regloss :  18402.210859
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  7.371065 ,  gradY :  -0.581248 ,  regloss :  17985.644249

   time:  1.74

 Ensemble Average :  21
 time: 0.81

 **** n =  21 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  24764

 Evaluate Y and gradY, timestep-wise : 


************** n =  20  **************


 **** n =  20 , Pre-processing 

 **** n =  20 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  7.403376 ,  gradY :  -0.581096 ,  regloss :  23146.745227
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  7.097404 ,  gradY :  -0.589387 ,  regloss :  17351.643809
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  6.664697 ,  gradY :  -0.590091 ,  regloss :  21647.809533
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  6.500892 ,  gradY :  -0.591290 ,  regloss :  22359.661572
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  6.406218 ,  gradY :  -0.569381 ,  regloss :  19852.305915
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  6.264415 ,  gradY :  -0.566647 ,  regloss :  20019.013404
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  6.302015 ,  gradY :  -0.578729 ,  regloss :  19560.162523
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  6.339483 ,  gradY :  -0.562239 ,  regloss :  20193.957960
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  6.590409 ,  gradY :  -0.603651 ,  regloss :  19867.573959
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  6.416640 ,  gradY :  -0.581500 ,  regloss :  23098.595428
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  6.332859 ,  gradY :  -0.577671 ,  regloss :  18662.243765
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  6.291919 ,  gradY :  -0.596158 ,  regloss :  18288.865922
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  6.248589 ,  gradY :  -0.582768 ,  regloss :  17918.784850
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  6.270087 ,  gradY :  -0.582910 ,  regloss :  17627.525687
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  6.430537 ,  gradY :  -0.588883 ,  regloss :  19637.356901
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  6.432518 ,  gradY :  -0.607446 ,  regloss :  20090.492513
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  6.475392 ,  gradY :  -0.576172 ,  regloss :  22379.761265
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  6.585562 ,  gradY :  -0.601523 ,  regloss :  17770.484983
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  6.469177 ,  gradY :  -0.573968 ,  regloss :  21460.717044
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  6.488569 ,  gradY :  -0.566341 ,  regloss :  18473.601202
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  6.347375 ,  gradY :  -0.582105 ,  regloss :  18899.708776
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.359026 ,  gradY :  -0.598366 ,  regloss :  20569.871292
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.461132 ,  gradY :  -0.589917 ,  regloss :  21137.419440
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.208017 ,  gradY :  -0.592162 ,  regloss :  22644.842315
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.690624 ,  gradY :  -0.593849 ,  regloss :  19339.531532

   time:  1.86

 Ensemble Average :  1
 time: 1.37

 Ensemble Average :  2
 time: 0.95

 Ensemble Average :  3
 time: 0.93

 Ensemble Average :  4
 time: 0.84

 Ensemble Average :  5
 time: 0.80

 Ensemble Average :  6
 time: 0.81

 Ensemble Average :  7
 time: 0.83

 Ensemble Average :  8
 time: 0.82

 Ensemble Average :  9
 time: 0.80

 Ensemble Average :  10
 time: 0.81

 Ensemble Average :  11
 time: 0.82

 Ensemble Average :  12
 time: 0.83

 Ensemble Average :  13
 time: 0.80

 Ensemble Average :  14
 time: 1.14

 Ensemble Average :  15
 time: 0.87

 Ensemble Average :  16
 time: 0.80

 Ensemble Average :  17
 time: 0.80

 Ensemble Average :  18
 time: 0.81

 Ensemble Average :  19
 time: 0.80

 Ensemble Average :  20
 time: 0.78

 **** n =  20 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  22966

 Evaluate Y and gradY, timestep-wise : 


************** n =  19  **************


 **** n =  19 , Pre-processing 

 **** n =  19 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  6.661035 ,  gradY :  -0.588402 ,  regloss :  18402.299278
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  6.804881 ,  gradY :  -0.551462 ,  regloss :  21681.683513
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  6.876249 ,  gradY :  -0.578733 ,  regloss :  19442.497538
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  6.811147 ,  gradY :  -0.600009 ,  regloss :  23558.490395
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  6.984763 ,  gradY :  -0.595293 ,  regloss :  19993.068408
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  6.963568 ,  gradY :  -0.583091 ,  regloss :  21924.752861
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  7.034089 ,  gradY :  -0.579392 ,  regloss :  19438.771290
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  7.046338 ,  gradY :  -0.594367 ,  regloss :  21933.476936
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  7.027687 ,  gradY :  -0.559142 ,  regloss :  22237.795742
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  7.049218 ,  gradY :  -0.580118 ,  regloss :  18360.547539
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  7.089820 ,  gradY :  -0.573635 ,  regloss :  23421.895404
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  7.138267 ,  gradY :  -0.591295 ,  regloss :  18710.346458
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  7.132914 ,  gradY :  -0.575724 ,  regloss :  17917.517306
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  7.144485 ,  gradY :  -0.578341 ,  regloss :  19183.160601
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  7.119782 ,  gradY :  -0.580243 ,  regloss :  20111.465357
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  7.065136 ,  gradY :  -0.571482 ,  regloss :  18925.680063
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  6.982162 ,  gradY :  -0.605823 ,  regloss :  20827.308062
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  7.013537 ,  gradY :  -0.603536 ,  regloss :  21761.363993
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  6.949430 ,  gradY :  -0.578977 ,  regloss :  19214.208988
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  6.860928 ,  gradY :  -0.601783 ,  regloss :  22064.515444
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  6.873413 ,  gradY :  -0.572941 ,  regloss :  21019.034713
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.912600 ,  gradY :  -0.575362 ,  regloss :  22179.625800
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  7.162818 ,  gradY :  -0.559491 ,  regloss :  18611.707170
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.814647 ,  gradY :  -0.581503 ,  regloss :  20345.650973
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.827221 ,  gradY :  -0.583388 ,  regloss :  18687.580756

   time:  1.57

 Ensemble Average :  19
 time: 0.79

 **** n =  19 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  21627

 Evaluate Y and gradY, timestep-wise : 


************** n =  18  **************


 **** n =  18 , Pre-processing 

 **** n =  18 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  6.866471 ,  gradY :  -0.589005 ,  regloss :  22935.550524
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  6.658785 ,  gradY :  -0.593957 ,  regloss :  19531.529235
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  6.426098 ,  gradY :  -0.579715 ,  regloss :  19777.174726
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  6.365102 ,  gradY :  -0.598477 ,  regloss :  20803.077465
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  6.315394 ,  gradY :  -0.578595 ,  regloss :  22714.039365
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  6.372325 ,  gradY :  -0.579873 ,  regloss :  21901.581730
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  6.420570 ,  gradY :  -0.578078 ,  regloss :  20666.244372
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  6.270914 ,  gradY :  -0.583222 ,  regloss :  23388.246442
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  6.233274 ,  gradY :  -0.576333 ,  regloss :  23400.208566
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  6.320778 ,  gradY :  -0.551196 ,  regloss :  19635.931382
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  6.263946 ,  gradY :  -0.591235 ,  regloss :  20494.217422
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  6.286020 ,  gradY :  -0.591260 ,  regloss :  20574.795208
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  6.222382 ,  gradY :  -0.591347 ,  regloss :  22419.013269
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  6.228956 ,  gradY :  -0.582401 ,  regloss :  20674.494429
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  6.303379 ,  gradY :  -0.594719 ,  regloss :  22702.284438
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  6.326633 ,  gradY :  -0.567300 ,  regloss :  22714.254480
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  6.472868 ,  gradY :  -0.580651 ,  regloss :  19645.747157
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  6.276699 ,  gradY :  -0.590978 ,  regloss :  22386.771152
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  6.364501 ,  gradY :  -0.563462 ,  regloss :  21558.837196
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  6.298714 ,  gradY :  -0.600420 ,  regloss :  21774.436396
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  6.477787 ,  gradY :  -0.573658 ,  regloss :  19403.687002
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.351598 ,  gradY :  -0.593323 ,  regloss :  17927.736316
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.614040 ,  gradY :  -0.588770 ,  regloss :  22952.810953
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.559039 ,  gradY :  -0.555481 ,  regloss :  23374.805078
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.895950 ,  gradY :  -0.583794 ,  regloss :  21771.037661

   time:  1.57

 Ensemble Average :  1
 time: 0.80

 Ensemble Average :  2
 time: 0.78

 Ensemble Average :  3
 time: 0.81

 Ensemble Average :  4
 time: 0.79

 Ensemble Average :  5
 time: 0.81

 Ensemble Average :  6
 time: 0.79

 Ensemble Average :  7
 time: 0.80

 Ensemble Average :  8
 time: 0.80

 Ensemble Average :  9
 time: 0.81

 Ensemble Average :  10
 time: 0.79

 Ensemble Average :  11
 time: 0.80

 Ensemble Average :  12
 time: 0.78

 Ensemble Average :  13
 time: 0.82

 Ensemble Average :  14
 time: 0.81

 Ensemble Average :  15
 time: 0.80

 Ensemble Average :  16
 time: 0.79

 Ensemble Average :  17
 time: 0.81

 Ensemble Average :  18
 time: 0.78

 **** n =  18 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  19569

 Evaluate Y and gradY, timestep-wise : 


************** n =  17  **************


 **** n =  17 , Pre-processing 

 **** n =  17 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  6.922693 ,  gradY :  -0.567553 ,  regloss :  20726.244198
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  7.033465 ,  gradY :  -0.566772 ,  regloss :  23101.887061
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  7.093877 ,  gradY :  -0.579763 ,  regloss :  23455.908857
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  7.141169 ,  gradY :  -0.578540 ,  regloss :  19810.961097
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  7.138414 ,  gradY :  -0.593871 ,  regloss :  22809.709428
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  7.202364 ,  gradY :  -0.586955 ,  regloss :  22628.731343
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  7.215261 ,  gradY :  -0.590955 ,  regloss :  23419.799133
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  7.228632 ,  gradY :  -0.553806 ,  regloss :  24069.245649
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  7.329963 ,  gradY :  -0.554673 ,  regloss :  23435.370993
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  7.298601 ,  gradY :  -0.587029 ,  regloss :  22131.315951
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  7.176732 ,  gradY :  -0.605730 ,  regloss :  22980.763546
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  7.139549 ,  gradY :  -0.580120 ,  regloss :  23595.742521
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  7.100511 ,  gradY :  -0.574193 ,  regloss :  26114.980459
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  7.176411 ,  gradY :  -0.582137 ,  regloss :  24475.925440
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  7.102561 ,  gradY :  -0.580268 ,  regloss :  21170.034492
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  7.130594 ,  gradY :  -0.575774 ,  regloss :  21648.822153
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  7.156452 ,  gradY :  -0.572238 ,  regloss :  22682.535787
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  7.118538 ,  gradY :  -0.602447 ,  regloss :  21767.379845
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  7.168236 ,  gradY :  -0.575874 ,  regloss :  20987.834110
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  7.133137 ,  gradY :  -0.551808 ,  regloss :  20407.081610
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  7.147060 ,  gradY :  -0.596250 ,  regloss :  23067.778156
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.979504 ,  gradY :  -0.605762 ,  regloss :  22824.038863
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.938201 ,  gradY :  -0.581904 ,  regloss :  20856.423367
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.797628 ,  gradY :  -0.575321 ,  regloss :  25069.702578
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.686714 ,  gradY :  -0.565028 ,  regloss :  26594.476568

   time:  1.68

 Ensemble Average :  17
 time: 0.99

 **** n =  17 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  18079

 Evaluate Y and gradY, timestep-wise : 


************** n =  16  **************


 **** n =  16 , Pre-processing 

 **** n =  16 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  6.714679 ,  gradY :  -0.569821 ,  regloss :  19981.841432
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  6.425764 ,  gradY :  -0.583320 ,  regloss :  23252.282319
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  6.016729 ,  gradY :  -0.584525 ,  regloss :  22812.251615
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  6.009160 ,  gradY :  -0.589442 ,  regloss :  27659.804359
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  6.066000 ,  gradY :  -0.604834 ,  regloss :  27220.175906
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  6.012604 ,  gradY :  -0.571812 ,  regloss :  25075.407277
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.951906 ,  gradY :  -0.592915 ,  regloss :  20539.601717
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  6.088677 ,  gradY :  -0.574319 ,  regloss :  26703.253839
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  6.047129 ,  gradY :  -0.594720 ,  regloss :  21736.450126
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.982332 ,  gradY :  -0.559557 ,  regloss :  21490.577560
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.869055 ,  gradY :  -0.570290 ,  regloss :  23511.434035
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  5.987000 ,  gradY :  -0.583104 ,  regloss :  25809.140885
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  5.974152 ,  gradY :  -0.584386 ,  regloss :  25927.539898
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  6.014430 ,  gradY :  -0.585676 ,  regloss :  22094.754093
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  6.051688 ,  gradY :  -0.581865 ,  regloss :  21456.156475
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  6.030165 ,  gradY :  -0.591857 ,  regloss :  21713.571346
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  6.080949 ,  gradY :  -0.564070 ,  regloss :  24173.217786
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  6.093632 ,  gradY :  -0.585249 ,  regloss :  23515.829397
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  6.182579 ,  gradY :  -0.585359 ,  regloss :  24652.770872
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  6.141383 ,  gradY :  -0.574475 ,  regloss :  24505.032948
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  6.199154 ,  gradY :  -0.598427 ,  regloss :  23266.265760
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  5.989968 ,  gradY :  -0.576787 ,  regloss :  22077.067980
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.007418 ,  gradY :  -0.578665 ,  regloss :  25193.583539
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.513316 ,  gradY :  -0.582064 ,  regloss :  23248.126584
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.682056 ,  gradY :  -0.582559 ,  regloss :  21671.927389

   time:  1.73

 Ensemble Average :  1
 time: 0.83

 Ensemble Average :  2
 time: 0.81

 Ensemble Average :  3
 time: 0.80

 Ensemble Average :  4
 time: 0.85

 Ensemble Average :  5
 time: 0.82

 Ensemble Average :  6
 time: 0.84

 Ensemble Average :  7
 time: 0.80

 Ensemble Average :  8
 time: 0.81

 Ensemble Average :  9
 time: 0.81

 Ensemble Average :  10
 time: 0.81

 Ensemble Average :  11
 time: 0.85

 Ensemble Average :  12
 time: 0.82

 Ensemble Average :  13
 time: 0.81

 Ensemble Average :  14
 time: 0.83

 Ensemble Average :  15
 time: 0.82

 Ensemble Average :  16
 time: 0.80

 **** n =  16 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  15845

 Evaluate Y and gradY, timestep-wise : 


************** n =  15  **************


 **** n =  15 , Pre-processing 

 **** n =  15 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  6.662153 ,  gradY :  -0.567921 ,  regloss :  26190.842077
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  6.714249 ,  gradY :  -0.552820 ,  regloss :  24005.103013
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  6.696010 ,  gradY :  -0.573533 ,  regloss :  24045.979839
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  6.724457 ,  gradY :  -0.566860 ,  regloss :  26049.955180
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  6.764213 ,  gradY :  -0.580206 ,  regloss :  27051.887612
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  6.822099 ,  gradY :  -0.575141 ,  regloss :  22772.996806
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  6.857940 ,  gradY :  -0.579446 ,  regloss :  23026.439233
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  6.814470 ,  gradY :  -0.575848 ,  regloss :  26147.149889
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  6.805352 ,  gradY :  -0.583384 ,  regloss :  23889.283190
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  6.791477 ,  gradY :  -0.576045 ,  regloss :  24310.954779
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  6.776390 ,  gradY :  -0.575261 ,  regloss :  27001.707249
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  6.815931 ,  gradY :  -0.579435 ,  regloss :  25990.062110
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  6.836496 ,  gradY :  -0.565559 ,  regloss :  26893.349119
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  6.789334 ,  gradY :  -0.570825 ,  regloss :  20514.408995
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  6.844708 ,  gradY :  -0.588091 ,  regloss :  24848.460985
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  6.816175 ,  gradY :  -0.563294 ,  regloss :  22696.401878
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  6.796037 ,  gradY :  -0.585748 ,  regloss :  24752.896409
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  6.883464 ,  gradY :  -0.589572 ,  regloss :  26909.726318
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  6.912771 ,  gradY :  -0.576047 ,  regloss :  27453.187850
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  6.947694 ,  gradY :  -0.582731 ,  regloss :  23954.951315
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  6.933004 ,  gradY :  -0.577134 ,  regloss :  21590.248052
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.765721 ,  gradY :  -0.570091 ,  regloss :  26211.532787
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.726422 ,  gradY :  -0.573976 ,  regloss :  25506.423082
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.673974 ,  gradY :  -0.581553 ,  regloss :  22481.832899
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.654433 ,  gradY :  -0.568258 ,  regloss :  26138.601657

   time:  1.60

 Ensemble Average :  15
 time: 0.80

 **** n =  15 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  15437

 Evaluate Y and gradY, timestep-wise : 


************** n =  14  **************


 **** n =  14 , Pre-processing 

 **** n =  14 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  6.735891 ,  gradY :  -0.589646 ,  regloss :  27803.225239
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  6.622932 ,  gradY :  -0.580311 ,  regloss :  24055.582316
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  6.499426 ,  gradY :  -0.596283 ,  regloss :  24307.395016
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  6.256275 ,  gradY :  -0.606117 ,  regloss :  27806.455927
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  6.257856 ,  gradY :  -0.590606 ,  regloss :  22960.100547
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  6.044848 ,  gradY :  -0.594606 ,  regloss :  25718.921469
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.911734 ,  gradY :  -0.559552 ,  regloss :  24450.396536
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.961660 ,  gradY :  -0.597696 ,  regloss :  23540.665292
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  5.892703 ,  gradY :  -0.593823 ,  regloss :  27325.936385
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.828325 ,  gradY :  -0.581303 ,  regloss :  26909.081640
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.975669 ,  gradY :  -0.573685 ,  regloss :  25083.731990
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  6.018492 ,  gradY :  -0.577195 ,  regloss :  26134.986351
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  5.935443 ,  gradY :  -0.570582 ,  regloss :  25838.230960
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  5.953193 ,  gradY :  -0.567185 ,  regloss :  24815.896719
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  5.910479 ,  gradY :  -0.574316 ,  regloss :  22108.605131
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  5.905633 ,  gradY :  -0.571892 ,  regloss :  22457.246564
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  5.993801 ,  gradY :  -0.569528 ,  regloss :  22239.788063
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  5.932774 ,  gradY :  -0.576119 ,  regloss :  23664.719761
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  5.929602 ,  gradY :  -0.585952 ,  regloss :  23471.301571
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  5.950006 ,  gradY :  -0.578376 ,  regloss :  27254.554326
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  5.994464 ,  gradY :  -0.562223 ,  regloss :  21883.100108
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.156893 ,  gradY :  -0.570562 ,  regloss :  22780.233968
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.139249 ,  gradY :  -0.572031 ,  regloss :  26463.225450
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.081002 ,  gradY :  -0.566979 ,  regloss :  25324.331066
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.913001 ,  gradY :  -0.587797 ,  regloss :  26724.036919

   time:  1.63

 Ensemble Average :  1
 time: 0.80

 Ensemble Average :  2
 time: 0.83

 Ensemble Average :  3
 time: 0.80

 Ensemble Average :  4
 time: 0.84

 Ensemble Average :  5
 time: 0.83

 Ensemble Average :  6
 time: 0.80

 Ensemble Average :  7
 time: 0.80

 Ensemble Average :  8
 time: 0.89

 Ensemble Average :  9
 time: 0.87

 Ensemble Average :  10
 time: 0.83

 Ensemble Average :  11
 time: 1.02

 Ensemble Average :  12
 time: 0.89

 Ensemble Average :  13
 time: 0.80

 Ensemble Average :  14
 time: 0.79

 **** n =  14 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  14955

 Evaluate Y and gradY, timestep-wise : 


************** n =  13  **************


 **** n =  13 , Pre-processing 

 **** n =  13 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  6.116730 ,  gradY :  -0.581203 ,  regloss :  22776.975344
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  6.251922 ,  gradY :  -0.578083 ,  regloss :  24091.647051
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  6.298397 ,  gradY :  -0.595097 ,  regloss :  24374.152759
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  6.398200 ,  gradY :  -0.559680 ,  regloss :  24258.391888
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  6.427319 ,  gradY :  -0.570438 ,  regloss :  25544.469743
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  6.413208 ,  gradY :  -0.587032 ,  regloss :  23622.150249
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  6.445236 ,  gradY :  -0.562152 ,  regloss :  22181.129883
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  6.475375 ,  gradY :  -0.570000 ,  regloss :  25721.478896
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  6.447548 ,  gradY :  -0.553241 ,  regloss :  24235.150012
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  6.398893 ,  gradY :  -0.593457 ,  regloss :  25139.237067
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  6.385724 ,  gradY :  -0.569227 ,  regloss :  21901.122711
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  6.352192 ,  gradY :  -0.576068 ,  regloss :  25590.669106
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  6.358498 ,  gradY :  -0.585167 ,  regloss :  22704.094181
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  6.319791 ,  gradY :  -0.575471 ,  regloss :  21922.338447
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  6.365177 ,  gradY :  -0.580905 ,  regloss :  24028.224713
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  6.433751 ,  gradY :  -0.570551 ,  regloss :  25075.524129
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  6.404917 ,  gradY :  -0.603571 ,  regloss :  22748.113864
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  6.431168 ,  gradY :  -0.585026 ,  regloss :  28227.206373
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  6.410248 ,  gradY :  -0.592357 ,  regloss :  27230.579353
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  6.447767 ,  gradY :  -0.564680 ,  regloss :  25373.961076
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  6.477491 ,  gradY :  -0.592509 ,  regloss :  28195.132788
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.544354 ,  gradY :  -0.567367 ,  regloss :  24851.232996
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.438730 ,  gradY :  -0.589637 ,  regloss :  25261.248862
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.719107 ,  gradY :  -0.593435 ,  regloss :  24465.035884
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.667020 ,  gradY :  -0.565870 ,  regloss :  23696.262151

   time:  1.62

 Ensemble Average :  13
 time: 0.80

 **** n =  13 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  14047

 Evaluate Y and gradY, timestep-wise : 


************** n =  12  **************


 **** n =  12 , Pre-processing 

 **** n =  12 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  6.587240 ,  gradY :  -0.582035 ,  regloss :  26307.732725
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  6.478932 ,  gradY :  -0.582543 ,  regloss :  24534.102491
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  6.321986 ,  gradY :  -0.582131 ,  regloss :  28610.950680
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  6.334329 ,  gradY :  -0.572994 ,  regloss :  27539.198164
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  6.098146 ,  gradY :  -0.574293 ,  regloss :  25229.862268
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.945109 ,  gradY :  -0.569828 ,  regloss :  24284.782587
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.966843 ,  gradY :  -0.575085 ,  regloss :  25623.652489
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.936435 ,  gradY :  -0.592083 ,  regloss :  25995.843924
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  6.105255 ,  gradY :  -0.572451 ,  regloss :  25833.247607
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  6.125877 ,  gradY :  -0.583025 ,  regloss :  26645.684665
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  6.193900 ,  gradY :  -0.575115 ,  regloss :  24917.507795
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  6.100308 ,  gradY :  -0.592472 ,  regloss :  23522.579569
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  6.133351 ,  gradY :  -0.577005 ,  regloss :  26316.193647
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  6.117801 ,  gradY :  -0.571970 ,  regloss :  27129.277224
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  6.080268 ,  gradY :  -0.597882 ,  regloss :  26145.664823
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  6.062667 ,  gradY :  -0.587003 ,  regloss :  26727.292545
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  6.014909 ,  gradY :  -0.548194 ,  regloss :  24526.383756
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  5.934261 ,  gradY :  -0.572142 ,  regloss :  24748.103111
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  5.890284 ,  gradY :  -0.568360 ,  regloss :  24017.695334
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  5.852326 ,  gradY :  -0.595086 ,  regloss :  25153.125036
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  5.950534 ,  gradY :  -0.577518 ,  regloss :  25894.802337
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.030935 ,  gradY :  -0.576893 ,  regloss :  26883.759997
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.965888 ,  gradY :  -0.576180 ,  regloss :  26084.834798
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.780362 ,  gradY :  -0.576282 ,  regloss :  23621.714982
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.681265 ,  gradY :  -0.587691 ,  regloss :  25591.914746

   time:  1.69

 Ensemble Average :  1
 time: 0.82

 Ensemble Average :  2
 time: 0.81

 Ensemble Average :  3
 time: 0.96

 Ensemble Average :  4
 time: 0.97

 Ensemble Average :  5
 time: 0.93

 Ensemble Average :  6
 time: 1.23

 Ensemble Average :  7
 time: 1.04

 Ensemble Average :  8
 time: 1.06

 Ensemble Average :  9
 time: 0.93

 Ensemble Average :  10
 time: 1.04

 Ensemble Average :  11
 time: 0.80

 Ensemble Average :  12
 time: 0.82

 **** n =  12 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  13208

 Evaluate Y and gradY, timestep-wise : 


************** n =  11  **************


 **** n =  11 , Pre-processing 

 **** n =  11 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  5.693809 ,  gradY :  -0.562358 ,  regloss :  29495.327992
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  5.739922 ,  gradY :  -0.574824 ,  regloss :  28299.264316
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.886769 ,  gradY :  -0.582516 ,  regloss :  26094.464896
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  6.009199 ,  gradY :  -0.558285 ,  regloss :  25922.748645
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.971533 ,  gradY :  -0.584481 ,  regloss :  23927.916901
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.977979 ,  gradY :  -0.576465 ,  regloss :  24924.837902
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  6.023619 ,  gradY :  -0.593633 ,  regloss :  21081.297595
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.992444 ,  gradY :  -0.576317 ,  regloss :  26683.318112
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  6.022084 ,  gradY :  -0.585576 ,  regloss :  25634.274796
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  6.143155 ,  gradY :  -0.569252 ,  regloss :  28943.999288
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  6.240566 ,  gradY :  -0.581991 ,  regloss :  25594.828051
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  6.242133 ,  gradY :  -0.568899 ,  regloss :  27625.484366
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  6.223698 ,  gradY :  -0.562185 ,  regloss :  23210.396053
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  6.197366 ,  gradY :  -0.570978 ,  regloss :  24190.450908
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  6.310675 ,  gradY :  -0.566369 ,  regloss :  24975.258758
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  6.286920 ,  gradY :  -0.573081 ,  regloss :  25111.513954
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  6.333278 ,  gradY :  -0.581266 ,  regloss :  24769.643625
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  6.291392 ,  gradY :  -0.584979 ,  regloss :  25989.206496
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  6.261235 ,  gradY :  -0.580301 ,  regloss :  25706.813207
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  6.177955 ,  gradY :  -0.557654 ,  regloss :  26527.491917
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  6.138269 ,  gradY :  -0.583039 ,  regloss :  27872.787733
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.215613 ,  gradY :  -0.597547 ,  regloss :  27421.799506
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.358170 ,  gradY :  -0.578227 ,  regloss :  25291.551352
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.185654 ,  gradY :  -0.579072 ,  regloss :  27805.752774
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.403773 ,  gradY :  -0.594327 ,  regloss :  22594.557381

   time:  1.68

 Ensemble Average :  11
 time: 0.82

 **** n =  11 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  11331

 Evaluate Y and gradY, timestep-wise : 


************** n =  10  **************


 **** n =  10 , Pre-processing 

 **** n =  10 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  6.357534 ,  gradY :  -0.582132 ,  regloss :  27737.535057
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  6.056688 ,  gradY :  -0.582966 ,  regloss :  26414.278602
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.857314 ,  gradY :  -0.590240 ,  regloss :  27251.037911
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  5.810917 ,  gradY :  -0.560376 ,  regloss :  24372.690233
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.724276 ,  gradY :  -0.596620 ,  regloss :  26731.605546
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.626742 ,  gradY :  -0.562576 ,  regloss :  24787.005479
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.638200 ,  gradY :  -0.573924 ,  regloss :  29041.765050
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.769656 ,  gradY :  -0.591871 ,  regloss :  25868.932588
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  5.829608 ,  gradY :  -0.586255 ,  regloss :  28245.027992
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.785909 ,  gradY :  -0.584867 ,  regloss :  26787.853048
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.845320 ,  gradY :  -0.577631 ,  regloss :  26761.494749
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  5.787742 ,  gradY :  -0.586657 ,  regloss :  29762.855843
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  5.800413 ,  gradY :  -0.578329 ,  regloss :  28548.370529
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  5.782174 ,  gradY :  -0.558957 ,  regloss :  25769.945762
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  5.818400 ,  gradY :  -0.590108 ,  regloss :  28279.109065
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  5.755947 ,  gradY :  -0.578282 ,  regloss :  26675.066712
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  5.739332 ,  gradY :  -0.574545 ,  regloss :  24297.063908
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  5.763871 ,  gradY :  -0.570027 ,  regloss :  26989.804850
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  5.842720 ,  gradY :  -0.580465 ,  regloss :  28270.970995
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  5.806160 ,  gradY :  -0.573903 ,  regloss :  26171.944868
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  5.689181 ,  gradY :  -0.565736 ,  regloss :  27458.114218
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  5.858981 ,  gradY :  -0.562494 ,  regloss :  24191.898773
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.972122 ,  gradY :  -0.569397 ,  regloss :  27682.655626
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.640136 ,  gradY :  -0.595925 ,  regloss :  27296.286073
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.683992 ,  gradY :  -0.576692 ,  regloss :  27287.392771

   time:  1.63

 Ensemble Average :  1
 time: 0.81

 Ensemble Average :  2
 time: 0.80

 Ensemble Average :  3
 time: 0.84

 Ensemble Average :  4
 time: 0.86

 Ensemble Average :  5
 time: 0.99

 Ensemble Average :  6
 time: 0.85

 Ensemble Average :  7
 time: 1.02

 Ensemble Average :  8
 time: 0.86

 Ensemble Average :  9
 time: 0.81

 Ensemble Average :  10
 time: 0.78

 **** n =  10 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  10519

 Evaluate Y and gradY, timestep-wise : 


************** n =  9  **************


 **** n =  9 , Pre-processing 

 **** n =  9 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  5.625719 ,  gradY :  -0.568921 ,  regloss :  25555.198003
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  5.703659 ,  gradY :  -0.564459 ,  regloss :  27031.925342
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.803845 ,  gradY :  -0.564494 ,  regloss :  28804.434244
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  5.890130 ,  gradY :  -0.581000 ,  regloss :  28617.002616
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.965205 ,  gradY :  -0.595649 ,  regloss :  27718.789159
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  6.023634 ,  gradY :  -0.579826 ,  regloss :  26147.060438
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  6.043509 ,  gradY :  -0.584774 ,  regloss :  29287.623394
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  6.066253 ,  gradY :  -0.586216 ,  regloss :  26065.588007
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  6.056769 ,  gradY :  -0.558509 ,  regloss :  25184.908556
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  6.081534 ,  gradY :  -0.571318 ,  regloss :  28025.516577
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  6.138424 ,  gradY :  -0.572668 ,  regloss :  26855.189653
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  6.178200 ,  gradY :  -0.577687 ,  regloss :  25832.266263
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  6.196086 ,  gradY :  -0.576791 ,  regloss :  25607.660237
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  6.125233 ,  gradY :  -0.576844 ,  regloss :  25371.301441
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  6.160604 ,  gradY :  -0.575621 ,  regloss :  28039.124259
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  6.225953 ,  gradY :  -0.572937 ,  regloss :  23999.976260
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  6.177021 ,  gradY :  -0.589274 ,  regloss :  30069.571445
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  6.175840 ,  gradY :  -0.585289 ,  regloss :  27054.924753
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  6.156839 ,  gradY :  -0.583292 ,  regloss :  28683.306068
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  6.247219 ,  gradY :  -0.573800 ,  regloss :  28547.611352
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  6.291594 ,  gradY :  -0.573117 ,  regloss :  26242.551191
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.367944 ,  gradY :  -0.564706 ,  regloss :  26137.330778
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.500786 ,  gradY :  -0.583076 ,  regloss :  26115.916269
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.321544 ,  gradY :  -0.553901 ,  regloss :  32092.422760
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.473476 ,  gradY :  -0.577577 ,  regloss :  28201.783319

   time:  1.63

 Ensemble Average :  9
 time: 0.79

 **** n =  9 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  8162

 Evaluate Y and gradY, timestep-wise : 


************** n =  8  **************


 **** n =  8 , Pre-processing 

 **** n =  8 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  6.545048 ,  gradY :  -0.578177 ,  regloss :  33563.108806
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  6.327769 ,  gradY :  -0.576174 ,  regloss :  28866.675292
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  6.162396 ,  gradY :  -0.574732 ,  regloss :  31671.130864
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  5.877414 ,  gradY :  -0.575312 ,  regloss :  29855.701421
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.942157 ,  gradY :  -0.577193 ,  regloss :  27249.157502
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.831536 ,  gradY :  -0.575088 ,  regloss :  29023.431069
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.717042 ,  gradY :  -0.571706 ,  regloss :  30683.612693
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.682178 ,  gradY :  -0.581372 ,  regloss :  25644.172642
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  5.668975 ,  gradY :  -0.588640 ,  regloss :  30001.888623
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.662839 ,  gradY :  -0.581165 ,  regloss :  30123.770576
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.598677 ,  gradY :  -0.571442 ,  regloss :  29545.145077
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  5.602772 ,  gradY :  -0.571484 ,  regloss :  26940.843479
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  5.574751 ,  gradY :  -0.564481 ,  regloss :  26623.249562
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  5.510841 ,  gradY :  -0.572123 ,  regloss :  27462.169004
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  5.499252 ,  gradY :  -0.576351 ,  regloss :  27912.110359
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  5.495010 ,  gradY :  -0.583996 ,  regloss :  28661.525244
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  5.632427 ,  gradY :  -0.573653 ,  regloss :  29072.849989
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  5.710865 ,  gradY :  -0.575064 ,  regloss :  30123.112704
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  5.749098 ,  gradY :  -0.576712 ,  regloss :  28916.624579
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  5.726032 ,  gradY :  -0.583400 ,  regloss :  25874.999223
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  5.770673 ,  gradY :  -0.576923 ,  regloss :  25465.697911
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  5.577113 ,  gradY :  -0.559252 ,  regloss :  26441.686368
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.608520 ,  gradY :  -0.563192 ,  regloss :  29416.684844
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.799768 ,  gradY :  -0.582892 ,  regloss :  29830.944526
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.848106 ,  gradY :  -0.584428 ,  regloss :  28092.158082

   time:  1.62

 Ensemble Average :  1
 time: 0.78

 Ensemble Average :  2
 time: 0.88

 Ensemble Average :  3
 time: 0.86

 Ensemble Average :  4
 time: 1.04

 Ensemble Average :  5
 time: 0.80

 Ensemble Average :  6
 time: 0.83

 Ensemble Average :  7
 time: 0.81

 Ensemble Average :  8
 time: 0.81

 **** n =  8 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  7001

 Evaluate Y and gradY, timestep-wise : 


************** n =  7  **************


 **** n =  7 , Pre-processing 

 **** n =  7 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  5.822291 ,  gradY :  -0.578449 ,  regloss :  28613.251119
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  5.838694 ,  gradY :  -0.574266 ,  regloss :  29444.774760
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.859659 ,  gradY :  -0.578202 ,  regloss :  30729.205281
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  5.841742 ,  gradY :  -0.555342 ,  regloss :  28008.854037
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.859611 ,  gradY :  -0.567486 ,  regloss :  28727.127014
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.905696 ,  gradY :  -0.579078 ,  regloss :  28959.012788
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.943766 ,  gradY :  -0.544187 ,  regloss :  30168.642125
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.962192 ,  gradY :  -0.582539 ,  regloss :  29675.357210
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  6.005352 ,  gradY :  -0.563994 ,  regloss :  30351.273766
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  6.043840 ,  gradY :  -0.563536 ,  regloss :  29871.643118
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  6.099833 ,  gradY :  -0.567898 ,  regloss :  28825.533352
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  6.123030 ,  gradY :  -0.567816 ,  regloss :  34416.188499
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  6.191139 ,  gradY :  -0.583351 ,  regloss :  30577.033117
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  6.276006 ,  gradY :  -0.582481 ,  regloss :  28516.372765
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  6.269774 ,  gradY :  -0.573053 ,  regloss :  30437.136146
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  6.250701 ,  gradY :  -0.573776 ,  regloss :  28646.802900
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  6.262580 ,  gradY :  -0.573531 ,  regloss :  32794.476283
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  6.276590 ,  gradY :  -0.571330 ,  regloss :  29592.079471
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  6.226953 ,  gradY :  -0.573594 ,  regloss :  28081.443925
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  6.156903 ,  gradY :  -0.572105 ,  regloss :  30939.562081
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  6.130679 ,  gradY :  -0.582870 ,  regloss :  30208.665907
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.211532 ,  gradY :  -0.564973 ,  regloss :  28331.775344
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.339899 ,  gradY :  -0.575806 ,  regloss :  30875.976869
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.390590 ,  gradY :  -0.568857 ,  regloss :  28650.593738
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.283305 ,  gradY :  -0.555407 ,  regloss :  28059.313188

   time:  1.67

 Ensemble Average :  7
 time: 0.82

 **** n =  7 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  5870

 Evaluate Y and gradY, timestep-wise : 


************** n =  6  **************


 **** n =  6 , Pre-processing 

 **** n =  6 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  6.307941 ,  gradY :  -0.582378 ,  regloss :  28767.665317
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  6.236093 ,  gradY :  -0.555806 ,  regloss :  30053.223630
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  6.279149 ,  gradY :  -0.572866 ,  regloss :  30698.510571
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  6.175123 ,  gradY :  -0.582275 ,  regloss :  28727.106171
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.940043 ,  gradY :  -0.580982 ,  regloss :  32656.846116
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.921519 ,  gradY :  -0.577619 ,  regloss :  33032.860569
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.895132 ,  gradY :  -0.570349 ,  regloss :  31294.759071
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.781966 ,  gradY :  -0.582733 ,  regloss :  32634.712294
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  5.696745 ,  gradY :  -0.578892 ,  regloss :  30354.061582
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.659192 ,  gradY :  -0.587774 ,  regloss :  29514.065362
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.786887 ,  gradY :  -0.569159 ,  regloss :  31845.261481
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  5.824942 ,  gradY :  -0.584526 ,  regloss :  32341.930178
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  5.841034 ,  gradY :  -0.564745 ,  regloss :  33351.936096
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  5.816302 ,  gradY :  -0.578309 ,  regloss :  28931.928832
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  5.788366 ,  gradY :  -0.567367 ,  regloss :  29410.627868
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  5.789114 ,  gradY :  -0.580711 ,  regloss :  34020.927222
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  5.789978 ,  gradY :  -0.576880 ,  regloss :  30822.217484
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  5.740054 ,  gradY :  -0.568602 ,  regloss :  32126.612943
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  5.663747 ,  gradY :  -0.576117 ,  regloss :  31919.385649
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  5.696883 ,  gradY :  -0.573405 ,  regloss :  31985.160583
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  5.726444 ,  gradY :  -0.572572 ,  regloss :  31390.183481
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  5.657882 ,  gradY :  -0.572349 ,  regloss :  30627.851570
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.663037 ,  gradY :  -0.575153 ,  regloss :  29913.305116
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.544926 ,  gradY :  -0.564672 ,  regloss :  29735.368244
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.720006 ,  gradY :  -0.574692 ,  regloss :  31069.245008

   time:  1.65

 Ensemble Average :  1
 time: 0.81

 Ensemble Average :  2
 time: 0.83

 Ensemble Average :  3
 time: 0.84

 Ensemble Average :  4
 time: 0.89

 Ensemble Average :  5
 time: 0.84

 Ensemble Average :  6
 time: 0.86

 **** n =  6 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  4433

 Evaluate Y and gradY, timestep-wise : 


************** n =  5  **************


 **** n =  5 , Pre-processing 

 **** n =  5 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  5.707350 ,  gradY :  -0.560579 ,  regloss :  29851.706490
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  5.728045 ,  gradY :  -0.568412 ,  regloss :  30822.330495
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.790682 ,  gradY :  -0.571475 ,  regloss :  30512.492162
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  5.839075 ,  gradY :  -0.556828 ,  regloss :  32851.127829
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.838044 ,  gradY :  -0.579950 ,  regloss :  31433.306547
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.862203 ,  gradY :  -0.572660 ,  regloss :  35596.674842
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.834378 ,  gradY :  -0.558757 ,  regloss :  29700.102140
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.841663 ,  gradY :  -0.573418 ,  regloss :  28366.597684
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  5.828525 ,  gradY :  -0.562359 ,  regloss :  32801.881819
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.908602 ,  gradY :  -0.582019 ,  regloss :  33516.216894
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.938067 ,  gradY :  -0.572567 ,  regloss :  31632.491153
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  5.919006 ,  gradY :  -0.576675 ,  regloss :  33166.257210
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  5.826122 ,  gradY :  -0.558999 ,  regloss :  30768.492166
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  5.854600 ,  gradY :  -0.574731 ,  regloss :  34109.336744
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  5.876480 ,  gradY :  -0.561362 ,  regloss :  32013.823832
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  5.920436 ,  gradY :  -0.564554 ,  regloss :  32363.380229
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  5.977967 ,  gradY :  -0.581256 ,  regloss :  31541.688388
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  6.037397 ,  gradY :  -0.561687 ,  regloss :  32790.706269
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  6.039846 ,  gradY :  -0.562687 ,  regloss :  29288.048762
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  6.017057 ,  gradY :  -0.564199 ,  regloss :  32839.769987
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  6.010594 ,  gradY :  -0.570528 ,  regloss :  33323.786308
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  6.246238 ,  gradY :  -0.553175 ,  regloss :  34300.135781
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.218478 ,  gradY :  -0.561778 ,  regloss :  31136.678483
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.264385 ,  gradY :  -0.574654 ,  regloss :  35670.564802
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  6.080605 ,  gradY :  -0.578917 ,  regloss :  34599.489954

   time:  1.65

 Ensemble Average :  5
 time: 0.80

 **** n =  5 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  3267

 Evaluate Y and gradY, timestep-wise : 


************** n =  4  **************


 **** n =  4 , Pre-processing 

 **** n =  4 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  6.134625 ,  gradY :  -0.568900 ,  regloss :  32757.777098
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  5.883924 ,  gradY :  -0.566048 ,  regloss :  31664.154113
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.710420 ,  gradY :  -0.572273 ,  regloss :  33391.356662
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  5.730861 ,  gradY :  -0.562733 ,  regloss :  31340.869122
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.664433 ,  gradY :  -0.573068 ,  regloss :  33404.340147
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.652408 ,  gradY :  -0.573235 ,  regloss :  31927.007846
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.649014 ,  gradY :  -0.571282 ,  regloss :  28872.807026
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.583026 ,  gradY :  -0.560729 ,  regloss :  31539.633008
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  5.533505 ,  gradY :  -0.560774 ,  regloss :  32993.290927
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.586168 ,  gradY :  -0.569745 ,  regloss :  31971.077500
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.636016 ,  gradY :  -0.556179 ,  regloss :  29753.957738
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  5.509454 ,  gradY :  -0.570970 ,  regloss :  30070.230988
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  5.429554 ,  gradY :  -0.564258 ,  regloss :  35512.225864
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  5.392841 ,  gradY :  -0.570191 ,  regloss :  33763.668809
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  5.502010 ,  gradY :  -0.571837 ,  regloss :  32633.319019
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  5.498649 ,  gradY :  -0.577546 ,  regloss :  32026.029110
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  5.550182 ,  gradY :  -0.581222 ,  regloss :  31066.825065
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  5.578891 ,  gradY :  -0.567455 ,  regloss :  32228.669562
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  5.499389 ,  gradY :  -0.568494 ,  regloss :  33013.862399
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  5.390759 ,  gradY :  -0.558832 ,  regloss :  31738.673820
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  5.457133 ,  gradY :  -0.570265 ,  regloss :  30031.851397
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  5.314273 ,  gradY :  -0.570799 ,  regloss :  33943.954759
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.354250 ,  gradY :  -0.581998 ,  regloss :  31098.657666
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.422162 ,  gradY :  -0.570706 ,  regloss :  33281.268911
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.447963 ,  gradY :  -0.566178 ,  regloss :  36407.713122

   time:  1.63

 Ensemble Average :  1
 time: 0.82

 Ensemble Average :  2
 time: 0.95

 Ensemble Average :  3
 time: 0.90

 Ensemble Average :  4
 time: 0.81

 **** n =  4 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  1727

 Evaluate Y and gradY, timestep-wise : 


************** n =  3  **************


 **** n =  3 , Pre-processing 

 **** n =  3 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  5.434717 ,  gradY :  -0.558476 ,  regloss :  36076.782487
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  5.482720 ,  gradY :  -0.560346 ,  regloss :  31072.120218
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.474896 ,  gradY :  -0.558731 ,  regloss :  34937.129134
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  5.543548 ,  gradY :  -0.558488 ,  regloss :  32381.297502
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.560036 ,  gradY :  -0.559467 ,  regloss :  33968.200660
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.602852 ,  gradY :  -0.559597 ,  regloss :  30815.568426
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.570728 ,  gradY :  -0.563120 ,  regloss :  35547.399271
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.577740 ,  gradY :  -0.575883 ,  regloss :  35214.998619
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  5.634458 ,  gradY :  -0.565546 ,  regloss :  33519.051123
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.666099 ,  gradY :  -0.568161 ,  regloss :  32953.725954
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.638960 ,  gradY :  -0.556398 ,  regloss :  35237.730939
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  5.676344 ,  gradY :  -0.550664 ,  regloss :  33197.375459
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  5.709540 ,  gradY :  -0.561630 ,  regloss :  31378.952136
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  5.658513 ,  gradY :  -0.568266 ,  regloss :  34585.840557
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  5.651496 ,  gradY :  -0.555280 ,  regloss :  34361.845062
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  5.648835 ,  gradY :  -0.559117 ,  regloss :  32445.788533
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  5.669945 ,  gradY :  -0.568248 ,  regloss :  28596.497522
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  5.651071 ,  gradY :  -0.572387 ,  regloss :  31341.070424
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  5.676019 ,  gradY :  -0.564917 ,  regloss :  32963.122789
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  5.708443 ,  gradY :  -0.560523 ,  regloss :  32152.404859
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  5.715367 ,  gradY :  -0.569315 ,  regloss :  34044.236402
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  5.802171 ,  gradY :  -0.566436 ,  regloss :  32223.848457
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.757139 ,  gradY :  -0.564327 ,  regloss :  32429.319293
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.568424 ,  gradY :  -0.563953 ,  regloss :  31074.603914
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.690390 ,  gradY :  -0.562994 ,  regloss :  32770.341587

   time:  1.75

 Ensemble Average :  3
 time: 0.80

 **** n =  3 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  701

 Evaluate Y and gradY, timestep-wise : 


************** n =  2  **************


 **** n =  2 , Pre-processing 

 **** n =  2 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  5.719766 ,  gradY :  -0.569697 ,  regloss :  31346.917292
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  5.728895 ,  gradY :  -0.567556 ,  regloss :  29215.808900
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.619560 ,  gradY :  -0.571335 ,  regloss :  30886.623419
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  5.593087 ,  gradY :  -0.557125 ,  regloss :  36060.142618
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.679131 ,  gradY :  -0.569882 ,  regloss :  37136.239692
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.565899 ,  gradY :  -0.571610 ,  regloss :  36768.191240
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.483908 ,  gradY :  -0.560918 ,  regloss :  33755.334678
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.397297 ,  gradY :  -0.561653 ,  regloss :  35266.506135
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  5.234594 ,  gradY :  -0.553121 ,  regloss :  27375.449236
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.105354 ,  gradY :  -0.559739 ,  regloss :  32453.692488
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.020458 ,  gradY :  -0.564686 ,  regloss :  34041.301982
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  5.013607 ,  gradY :  -0.564764 ,  regloss :  33226.000421
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  4.939698 ,  gradY :  -0.552703 ,  regloss :  34118.873578
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  4.976844 ,  gradY :  -0.568651 ,  regloss :  39151.245198
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  4.948714 ,  gradY :  -0.563068 ,  regloss :  33323.983605
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  4.944676 ,  gradY :  -0.572179 ,  regloss :  33383.883095
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  4.936102 ,  gradY :  -0.560346 ,  regloss :  32290.079848
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  4.988558 ,  gradY :  -0.560433 ,  regloss :  33284.410552
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  4.908343 ,  gradY :  -0.570280 ,  regloss :  32836.071008
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  4.924462 ,  gradY :  -0.572459 ,  regloss :  34944.158785
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  4.904122 ,  gradY :  -0.573925 ,  regloss :  35480.128118
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  5.120943 ,  gradY :  -0.567116 ,  regloss :  31946.265665
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.108277 ,  gradY :  -0.560778 ,  regloss :  32000.074284
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.202294 ,  gradY :  -0.569654 ,  regloss :  32689.071331
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.238710 ,  gradY :  -0.568416 ,  regloss :  31565.838155

   time:  1.63

 Ensemble Average :  1
 time: 0.82

 Ensemble Average :  2
 time: 0.80

 **** n =  2 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  159

 Evaluate Y and gradY, timestep-wise : 


************** n =  1  **************


 **** n =  1 , Pre-processing 

 **** n =  1 , Neural Network 

 ****  AmerOp , Training : 

 ****  AmerOp , Adam Optimization : 
 step :     1 ,  learning rate:  0.010000 ,  batch learning rate:  1.000000 ,  b :  5.211511 ,  gradY :  -0.553628 ,  regloss :  32912.577834
 step :     2 ,  learning rate:  0.010000 ,  batch learning rate:  0.870834 ,  b :  5.217501 ,  gradY :  -0.563057 ,  regloss :  35836.043022
 step :     3 ,  learning rate:  0.010000 ,  batch learning rate:  0.758336 ,  b :  5.190516 ,  gradY :  -0.554895 ,  regloss :  31579.393197
 step :     4 ,  learning rate:  0.010000 ,  batch learning rate:  0.660354 ,  b :  5.190209 ,  gradY :  -0.553110 ,  regloss :  34250.109707
 step :     5 ,  learning rate:  0.010000 ,  batch learning rate:  0.575015 ,  b :  5.230515 ,  gradY :  -0.556900 ,  regloss :  35275.285587
 step :     6 ,  learning rate:  0.010000 ,  batch learning rate:  0.500688 ,  b :  5.238170 ,  gradY :  -0.563224 ,  regloss :  37889.681036
 step :     7 ,  learning rate:  0.008710 ,  batch learning rate:  0.435952 ,  b :  5.314134 ,  gradY :  -0.559705 ,  regloss :  36500.061323
 step :     8 ,  learning rate:  0.007586 ,  batch learning rate:  0.379569 ,  b :  5.379656 ,  gradY :  -0.562840 ,  regloss :  34694.075150
 step :     9 ,  learning rate:  0.006607 ,  batch learning rate:  0.330462 ,  b :  5.390449 ,  gradY :  -0.555959 ,  regloss :  33758.710895
 step :    10 ,  learning rate:  0.005754 ,  batch learning rate:  0.287691 ,  b :  5.389077 ,  gradY :  -0.570892 ,  regloss :  36233.980068
 step :    11 ,  learning rate:  0.005012 ,  batch learning rate:  0.250439 ,  b :  5.359784 ,  gradY :  -0.555527 ,  regloss :  33969.510951
 step :    12 ,  learning rate:  0.004365 ,  batch learning rate:  0.217994 ,  b :  5.398104 ,  gradY :  -0.559637 ,  regloss :  35495.883843
 step :    13 ,  learning rate:  0.003802 ,  batch learning rate:  0.189736 ,  b :  5.408751 ,  gradY :  -0.559418 ,  regloss :  31335.409282
 step :    14 ,  learning rate:  0.003311 ,  batch learning rate:  0.165124 ,  b :  5.414840 ,  gradY :  -0.559657 ,  regloss :  29971.444046
 step :    15 ,  learning rate:  0.002884 ,  batch learning rate:  0.143688 ,  b :  5.402209 ,  gradY :  -0.557173 ,  regloss :  36531.581407
 step :    16 ,  learning rate:  0.002512 ,  batch learning rate:  0.125018 ,  b :  5.450215 ,  gradY :  -0.556723 ,  regloss :  35265.002837
 step :    17 ,  learning rate:  0.002188 ,  batch learning rate:  0.108757 ,  b :  5.447047 ,  gradY :  -0.562389 ,  regloss :  33464.961389
 step :    18 ,  learning rate:  0.001905 ,  batch learning rate:  0.094594 ,  b :  5.477455 ,  gradY :  -0.560212 ,  regloss :  35077.605031
 step :    19 ,  learning rate:  0.001660 ,  batch learning rate:  0.082259 ,  b :  5.483855 ,  gradY :  -0.560050 ,  regloss :  33008.588567
 step :    20 ,  learning rate:  0.001445 ,  batch learning rate:  0.071515 ,  b :  5.435890 ,  gradY :  -0.555809 ,  regloss :  33280.626410
 step :    21 ,  learning rate:  0.001259 ,  batch learning rate:  0.062158 ,  b :  5.432144 ,  gradY :  -0.557174 ,  regloss :  34996.814626
 step :    41 ,  learning rate:  0.000079 ,  batch learning rate:  0.002984 ,  b :  5.478415 ,  gradY :  -0.559094 ,  regloss :  33361.382256
 step :    61 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.464582 ,  gradY :  -0.567174 ,  regloss :  35304.860276
 step :    81 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.437872 ,  gradY :  -0.558576 ,  regloss :  33393.861723
 step :   100 ,  learning rate:  0.000010 ,  batch learning rate:  0.000000 ,  b :  5.244276 ,  gradY :  -0.557892 ,  regloss :  37706.741306

   time:  1.65

 Ensemble Average :  1
 time: 0.79

 **** n =  1 , Post-processing 

 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  7

 Evaluate Y and gradY, timestep-wise : 


************** n =  0  **************


 **** n =  0 , Pre-processing 

 **** n =  0 , Evaluation 

************** Results (Neural Network) **************
 The point X0 to evaluate is:  [100.]
 The price at t=0 is:  15.098324 ;  95% CI: [ 15.073605 15.123044 ]
 The delta at t=0 is:  [-0.558507] 


 Evaluate control (exercise boundary) : 
  Number of in-the-money points :  0

 Evaluate Y and gradY, timestep-wise : 

Total running time :  278.18


************** Evaluation of the Entire Model, nu =  0  **************

************** Results (Simulation values) **************
 The point X0 to evaluate is:  [100.]
 The price at t=0 is:  15.070194 ;  95% CI: [ 14.977156 15.163232 ]
 The delta at t=0 is:  [-0.539374] 



************** Hedging Results (Simulation values) **************


 P & L mean:  -0.15635883632677497 , std:  0.16768316376811798
