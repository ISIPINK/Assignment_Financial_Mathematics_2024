
@article{grohs_deep_2022,
	title = {Deep neural network approximations for solutions of {PDEs} based on {Monte} {Carlo} algorithms},
	volume = {3},
	issn = {2662-2963, 2662-2971},
	url = {https://link.springer.com/10.1007/s42985-021-00100-z},
	doi = {10.1007/s42985-021-00100-z},
	abstract = {In the past few years deep artiﬁcial neural networks (DNNs) have been successfully employed in a large number of computational problems including, e.g., language processing, image recognition, fraud detection, and computational advertisement. Recently, it has also been proposed in the scientiﬁc literature to reformulate high-dimensional partial differential equations (PDEs) as stochastic learning problems and to employ DNNs together with stochastic gradient descent methods to approximate the solutions of such high-dimensional PDEs. There are also a few mathematical convergence results in the scientiﬁc literature which show that DNNs can approximate solutions of certain PDEs without the curse of dimensionality in the sense that the number of real parameters employed to describe the DNN grows at most polynomially both in the PDE dimension d ∈ N and the reciprocal of the prescribed approximation accuracy ε {\textgreater} 0. One key argument in most of these results is, ﬁrst, to employ a Monte Carlo approximation scheme which can approximate the solution of the PDE under consideration at a ﬁxed space-time point without the curse of dimensionality and, thereafter, to prove then that DNNs are ﬂexible enough to mimic the behaviour of the employed approximation scheme. Having this in mind, one could aim for a general abstract result which shows under suitable assumptions that if a certain function can be approximated by any kind of (Monte Carlo) approximation scheme without the curse of dimensionality, then the function can also be approximated with DNNs without the curse of dimensionality. It is a subject of this article to make a ﬁrst step towards this direction. In particular, the main result of this paper, roughly speaking, shows that if a function can be approximated by means of some suitable discrete approximation scheme without the curse of dimensionality and if there exist DNNs which satisfy certain regularity properties and which approximate this discrete approximation scheme without the curse of dimensionality, then the function itself can also be approximated with DNNs without the curse of dimensionality. Moreover, for the number of real parameters used to describe such approximating DNNs we provide an explicit upper bound for the optimal exponent of the dimension d ∈ N of the function under consideration as well as an explicit lower bound for the optimal exponent of the prescribed approximation accuracy ε {\textgreater} 0. As an application of this result we derive that solutions of suitable Kolmogorov PDEs can be approximated with DNNs without the curse of dimensionality.},
	language = {en},
	number = {4},
	urldate = {2024-01-18},
	journal = {Partial Differential Equations and Applications},
	author = {Grohs, Philipp and Jentzen, Arnulf and Salimova, Diyora},
	month = aug,
	year = {2022},
	keywords = {monte carlo, machine learning},
	pages = {45},
	file = {Grohs e.a. - 2022 - Deep neural network approximations for solutions o.pdf:C\:\\Users\\isido\\Zotero\\storage\\V3BZRWL6\\Grohs e.a. - 2022 - Deep neural network approximations for solutions o.pdf:application/pdf},
}

@misc{ma_transition-based_2018,
	title = {Transition-based versus {State}-based {Reward} {Functions} for {MDPs} with {Value}-at-{Risk}},
	url = {http://arxiv.org/abs/1612.02088},
	abstract = {In reinforcement learning, the reward function on current state and action is widely used. When the objective is about the expectation of the (discounted) total reward only, it works perfectly. However, if the objective involves the total reward distribution, the result will be wrong. This paper studies Value-at-Risk (VaR) problems in short- and long-horizon Markov decision processes (MDPs) with two reward functions, which share the same expectations. Firstly we show that with VaR objective, when the real reward function is transition-based (with respect to action and both current and next states), the simplified (state-based, with respect to action and current state only) reward function will change the VaR. Secondly, for long-horizon MDPs, we estimate the VaR function with the aid of spectral theory and the central limit theorem. Thirdly, since the estimation method is for a Markov reward process with the reward function on current state only, we present a transformation algorithm for the Markov reward process with the reward function on current and next states, in order to estimate the VaR function with an intact total reward distribution.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Ma, Shuai and Yu, Jia Yuan},
	month = nov,
	year = {2018},
	note = {arXiv:1612.02088 [cs]
version: 3},
	keywords = {finance, reinforcement learning},
	annote = {Comment: 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\CVVTZGUS\\1612.html:text/html;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\4YPXXZ53\\Ma en Yu - 2018 - Transition-based versus State-based Reward Functio.pdf:application/pdf},
}

@article{longstaff_valuing_2001,
	title = {Valuing {American} {Options} by {Simulation}: {A} {Simple} {Least}-{Squares} {Approach}},
	volume = {14},
	issn = {0893-9454, 1465-7368},
	shorttitle = {Valuing {American} {Options} by {Simulation}},
	url = {https://academic.oup.com/rfs/article-lookup/doi/10.1093/rfs/14.1.113},
	doi = {10.1093/rfs/14.1.113},
	language = {en},
	number = {1},
	urldate = {2024-03-23},
	journal = {Review of Financial Studies},
	author = {Longstaff, Francis A. and Schwartz, Eduardo S.},
	month = jan,
	year = {2001},
	keywords = {american option},
	pages = {113--147},
	file = {Longstaff en Schwartz - 2001 - Valuing American Options by Simulation A Simple L.pdf:C\:\\Users\\isido\\Zotero\\storage\\J77VHX24\\Longstaff en Schwartz - 2001 - Valuing American Options by Simulation A Simple L.pdf:application/pdf},
}

@misc{turkedjiev_least_2017,
	type = {video},
	title = {Least squares regression {Monte} {Carlo} for approximating {BSDES} and semilinear {PDES}},
	copyright = {CC BY NC ND},
	url = {http://library.cirm-math.fr/Record.htm?idlist=5&record=19283916124910011989},
	abstract = {In this lecture, we shall discuss the key steps involved in the use of least squares regression for approximating the solution to BSDEs. This includes how to obtain explicit error estimates, and how these error estimates can be used to tune the parameters of the numerical scheme based on complexity considerations.{\textless}br{\textgreater}The algorithms are based on a two stage approximation process. Firstly, a suitable discrete time process is chosen to approximate the of the continuous time solution of the BSDE. The nodes of the discrete time processes can be expressed as conditional expectations. As we shall demonstrate, the choice of discrete time process is very important, as its properties will impact the performance of the overall numerical scheme. In the second stage, the conditional expectation is approximated in functional form using least squares regression on synthetically generated data – Monte Carlo simulations drawn from a suitable probability distribution. A key feature of the regression step is that the explanatory variables are built on a user chosen finite dimensional linear space of functions, which the user specifies by setting basis functions. The choice of basis functions is made on the hypothesis that it contains the solution, so regularity and boundedness assumptions are used in its construction. The impact of the choice of the basis functions is exposed in error estimates.{\textless}br{\textgreater}In addition to the choice of discrete time approximation and the basis functions, the Markovian structure of the problem gives significant additional freedom with regards to the Monte Carlo simulations. We demonstrate how to use this additional freedom to develop generic stratified sampling approaches that are independent of the underlying transition density function. Moreover, we demonstrate how to leverage the stratification method to develop a HPC algorithm for implementation on GPUs.{\textless}br{\textgreater}Thanks to the Feynmann-Kac relation between the the solution of a BSDE and its associated semilinear PDE, the approximation of the BSDE can be directly used to approximate the solution of the PDE. Moreover, the smoothness properties of the PDE play a crucial role in the selection of the hypothesis space of regressions functions, so this relationship is vitally important for the numerical scheme.{\textless}br{\textgreater}We conclude with some draw backs of the regression approach, notably the curse of dimensionality.},
	language = {Anglais},
	urldate = {2024-03-24},
	journal = {CEMRACS - Summer school: Numerical methods for stochastic models: control, uncertainty quantification, mean-field;CEMRACS - École d'été : Méthodes numériques pour équations stochastiques : contrôle, incertitude, champ moyen ; http://conferences.cirm-math.fr/1556.html},
	author = {Turkedjiev, Plamen},
	month = jul,
	year = {2017},
	note = {Publisher: CIRM},
}

@misc{wang_deep_2022,
	title = {Deep {BSDE}-{ML} {Learning} and {Its} {Application} to {Model}-{Free} {Optimal} {Control}},
	url = {http://arxiv.org/abs/2201.01318},
	abstract = {A modified Deep BSDE (backward differential equation) learning method with measurability loss, called Deep BSDE-ML method, is introduced in this paper to solve a kind of linear decoupled forward-backward stochastic differential equations (FBSDEs), which is encountered in the policy evaluation of learning the optimal feedback policies of a class of stochastic control problems. The measurability loss is characterized via the measurability of BSDE's state at the forward initial time, which differs from that related to terminal state of the known Deep BSDE method. Though the minima of the two loss functions are shown to be equal, this measurability loss is proved to be equal to the expected mean squared error between the true diffusion term of BSDE and its approximation. This crucial observation extends the application of the Deep BSDE method -- approximating the gradients of the solution of a partial differential equation (PDE) instead of the solution itself. Simultaneously, a learning-based framework is introduced to search an optimal feedback control of a deterministic nonlinear system. Specifically, by introducing Gaussian exploration noise, we are aiming to learn a robust optimal controller under this stochastic case. This reformulation sacrifices the optimality to some extent, but as suggested in reinforcement learning (RL) exploration noise is essential to enable the model-free learning.},
	urldate = {2024-03-26},
	publisher = {arXiv},
	author = {Wang, Yutian and Ni, Yuan-Hua},
	month = jan,
	year = {2022},
	note = {arXiv:2201.01318 [math]},
	keywords = {deep learning, bsde},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\6IRNFHSB\\2201.html:text/html;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\KR3U4MVG\\Wang en Ni - 2022 - Deep BSDE-ML Learning and Its Application to Model.pdf:application/pdf},
}

@article{e_deep_2017,
	title = {Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations},
	volume = {5},
	issn = {2194-6701, 2194-671X},
	url = {http://arxiv.org/abs/1706.04702},
	doi = {10.1007/s40304-017-0117-6},
	abstract = {We propose a new algorithm for solving parabolic partial differential equations (PDEs) and backward stochastic differential equations (BSDEs) in high dimension, by making an analogy between the BSDE and reinforcement learning with the gradient of the solution playing the role of the policy function, and the loss function given by the error between the prescribed terminal condition and the solution of the BSDE. The policy function is then approximated by a neural network, as is done in deep reinforcement learning. Numerical results using TensorFlow illustrate the efficiency and accuracy of the proposed algorithms for several 100-dimensional nonlinear PDEs from physics and finance such as the Allen-Cahn equation, the Hamilton-Jacobi-Bellman equation, and a nonlinear pricing model for financial derivatives.},
	number = {4},
	urldate = {2024-03-26},
	journal = {Communications in Mathematics and Statistics},
	author = {E, Weinan and Han, Jiequn and Jentzen, Arnulf},
	month = dec,
	year = {2017},
	note = {arXiv:1706.04702 [cs, math, stat]},
	keywords = {PDE, deep learning},
	pages = {349--380},
	annote = {Comment: 39 pages, 15 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\DXEZK8Z5\\1706.html:text/html;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\KSK3VRY6\\E e.a. - 2017 - Deep learning-based numerical methods for high-dim.pdf:application/pdf},
}

@article{turkedjiev_least-square_nodate,
	title = {Least-square regression {Monte} {Carlo} for approximating {BSDEs} and semilinear {PDEs}},
	language = {en},
	author = {Turkedjiev, Plamen},
	keywords = {PDE, bsde},
	file = {Turkedjiev - Least-square regression Monte Carlo for approximat.pdf:C\:\\Users\\isido\\Zotero\\storage\\ZBJXBW2U\\Turkedjiev - Least-square regression Monte Carlo for approximat.pdf:application/pdf},
}

@article{han_solving_2018,
	title = {Solving high-dimensional partial differential equations using deep learning},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1707.02568},
	doi = {10.1073/pnas.1718942115},
	abstract = {Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the "curse of dimensionality". This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black-Scholes equation, the Hamilton-Jacobi-Bellman equation, and the Allen-Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up new possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their inter-relationships.},
	number = {34},
	urldate = {2024-04-01},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
	month = aug,
	year = {2018},
	note = {arXiv:1707.02568 [cs, math]},
	keywords = {PDE, deep learning},
	pages = {8505--8510},
	annote = {Comment: 13 pages, 6 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\LEAWXB4F\\1707.html:text/html;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\5FH5LMXT\\Han e.a. - 2018 - Solving high-dimensional partial differential equa.pdf:application/pdf},
}

@article{klimsiak_valuing_2016,
	title = {Valuing {American} options by simulation: {A} {BSDEs} approach},
	volume = {123},
	issn = {03784754},
	shorttitle = {Valuing {American} options by simulation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378475415002724},
	doi = {10.1016/j.matcom.2015.11.009},
	abstract = {We provide probabilistic proofs of convergence of several easy to implement schemes for computing the value function of American (call and put) options written on a dividend paying stock governed by the geometric Brownian motion. The proofs are based on representations of the value function by means of solutions of some backward stochastic diﬀerential equations. Despite the probabilistic nature of the proofs the numerical schemes are nevertheless deterministic. Simulation results are also presented.},
	language = {en},
	urldate = {2024-04-01},
	journal = {Mathematics and Computers in Simulation},
	author = {Klimsiak, Tomasz and Rozkosz, Andrzej and Ziemkiewicz, Bartosz},
	month = may,
	year = {2016},
	keywords = {american option, bsde},
	pages = {1--18},
	file = {Klimsiak e.a. - 2016 - Valuing American options by simulation A BSDEs ap.pdf:C\:\\Users\\isido\\Zotero\\storage\\D3KFSMYS\\Klimsiak e.a. - 2016 - Valuing American options by simulation A BSDEs ap.pdf:application/pdf},
}

@misc{klimsiak_backward_2013,
	title = {On backward stochastic differential equations approach to valuation of {American} options},
	url = {http://arxiv.org/abs/1012.4442},
	abstract = {We consider the problem of valuation of American (call and put) options written on a dividend paying stock governed by the geometric Brownian motion. We show that the value function has two different but related representations: by means of a solution of some nonlinear backward stochastic differential equation and weak solution to some semilinear partial differential equation.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Klimsiak, Tomasz and Rozkosz, Andrzej},
	month = feb,
	year = {2013},
	note = {arXiv:1012.4442 [math]},
	keywords = {finance, american option, bsde},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\IFL2JSPM\\1012.html:text/html;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\MAU46LKV\\Klimsiak en Rozkosz - 2013 - On backward stochastic differential equations appr.pdf:application/pdf},
}

@article{klimsiak_early_2016,
	title = {The {Early} {Exercise} {Premium} {Representation} for {American} {Options} on {Multiply} {Assets}},
	volume = {73},
	issn = {0095-4616, 1432-0606},
	url = {http://link.springer.com/10.1007/s00245-015-9293-5},
	doi = {10.1007/s00245-015-9293-5},
	abstract = {In the paper we consider the problem of valuation of American options written on dividend-paying assets whose price dynamics follow the classical multidimensional Black and Scholes model. We provide a general early exercise premium representation formula for options with payoff functions which are convex or satisfy mild regularity assumptions. Examples include index options, spread options, call on max options, put on min options, multiply strike options and power-product options. In the proof of the formula we exploit close connections between the optimal stopping problems associated with valuation of American options, obstacle problems and reﬂected backward stochastic differential equations.},
	language = {en},
	number = {1},
	urldate = {2024-04-02},
	journal = {Applied Mathematics \& Optimization},
	author = {Klimsiak, Tomasz and Rozkosz, Andrzej},
	month = feb,
	year = {2016},
	keywords = {finance, american option},
	pages = {99--114},
	file = {Klimsiak en Rozkosz - 2016 - The Early Exercise Premium Representation for Amer.pdf:C\:\\Users\\isido\\Zotero\\storage\\S6RIVHJY\\Klimsiak en Rozkosz - 2016 - The Early Exercise Premium Representation for Amer.pdf:application/pdf},
}

@article{becker_solving_2021,
	title = {Solving high-dimensional optimal stopping problems using deep learning},
	volume = {32},
	issn = {0956-7925, 1469-4425},
	url = {http://arxiv.org/abs/1908.01602},
	doi = {10.1017/S0956792521000073},
	abstract = {Nowadays many financial derivatives, such as American or Bermudan options, are of early exercise type. Often the pricing of early exercise options gives rise to high-dimensional optimal stopping problems, since the dimension corresponds to the number of underlying assets. High-dimensional optimal stopping problems are, however, notoriously difficult to solve due to the well-known curse of dimensionality. In this work, we propose an algorithm for solving such problems, which is based on deep learning and computes, in the context of early exercise option pricing, both approximations of an optimal exercise strategy and the price of the considered option. The proposed algorithm can also be applied to optimal stopping problems that arise in other areas where the underlying stochastic process can be efficiently simulated. We present numerical results for a large number of example problems, which include the pricing of many high-dimensional American and Bermudan options, such as Bermudan max-call options in up to 5000 dimensions. Most of the obtained results are compared to reference values computed by exploiting the specific problem design or, where available, to reference values from the literature. These numerical results suggest that the proposed algorithm is highly effective in the case of many underlyings, in terms of both accuracy and speed.},
	number = {3},
	urldate = {2024-04-03},
	journal = {European Journal of Applied Mathematics},
	author = {Becker, Sebastian and Cheridito, Patrick and Jentzen, Arnulf and Welti, Timo},
	month = jun,
	year = {2021},
	note = {arXiv:1908.01602 [cs, math, q-fin]},
	keywords = {PDE},
	pages = {470--514},
	annote = {Comment: 54 pages, 1 figure},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\ANKENT9I\\1908.html:text/html;Becker e.a. - 2021 - Solving high-dimensional optimal stopping problems.pdf:C\:\\Users\\isido\\Zotero\\storage\\KDSQQELX\\Becker e.a. - 2021 - Solving high-dimensional optimal stopping problems.pdf:application/pdf;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\FCEWUK86\\Becker e.a. - 2021 - Solving high-dimensional optimal stopping problems.pdf:application/pdf},
}

@article{du_deep_2020,
	title = {Deep {Reinforcement} {Learning} for {Option} {Replication} and {Hedging}},
	volume = {2},
	issn = {2640-3943},
	url = {http://pm-research.com/lookup/doi/10.3905/jfds.2020.1.045},
	doi = {10.3905/jfds.2020.1.045},
	abstract = {The authors propose models for the solution of the fundamental problem of option replication subject to discrete trading, round lotting, and nonlinear transaction costs using state-of-the-art methods in deep reinforcement learning (DRL), including deep Q-learning, deep Q-learning with Pop-Art, and proximal policy optimization (PPO). Each DRL model is trained to hedge a whole range of strikes, and no retraining is needed when the user changes to another strike within the range. The models are general, allowing the user to plug in any option pricing and simulation library and then train them with no further modifications to hedge arbitrary option portfolios. Through a series of simulations, the authors show that the DRL models learn similar or better strategies as compared to delta hedging. Out of all models, PPO performs the best in terms of profit and loss, training time, and amount of data needed for training.},
	language = {en},
	number = {4},
	urldate = {2024-04-04},
	journal = {The Journal of Financial Data Science},
	author = {Du, Jiayi and Jin, Muyang and Kolm, Petter N. and Ritter, Gordon and Wang, Yixuan and Zhang, Bofei},
	month = oct,
	year = {2020},
	keywords = {finance, reinforcement learning, deep learning},
	pages = {44--57},
	file = {Du e.a. - 2020 - Deep Reinforcement Learning for Option Replication.pdf:C\:\\Users\\isido\\Zotero\\storage\\T5MUQU4C\\Du e.a. - 2020 - Deep Reinforcement Learning for Option Replication.pdf:application/pdf},
}

@article{buehler_deep_2019,
	title = {Deep {Hedging}: {Hedging} {Derivatives} {Under} {Generic} {Market} {Frictions} {Using} {Reinforcement} {Learning}},
	issn = {1556-5068},
	shorttitle = {Deep {Hedging}},
	url = {https://www.ssrn.com/abstract=3355706},
	doi = {10.2139/ssrn.3355706},
	abstract = {This article discusses a new application of reinforcement learning: to the problem of hedging a portfolio of “over-the-counter” derivatives under under market frictions such as trading costs and liquidity constraints. It is an extended version of our recent work [3], here using notation more common in the machine learning literature.},
	language = {en},
	urldate = {2024-04-04},
	journal = {SSRN Electronic Journal},
	author = {Buehler, Hans and Gonon, Lukas and Teichmann, Josef and Wood, Ben and Mohan, Baranidharan and Kochems, Jonathan},
	year = {2019},
	keywords = {finance, reinforcement learning, deep learning},
	file = {Buehler e.a. - 2019 - Deep Hedging Hedging Derivatives Under Generic Ma.pdf:C\:\\Users\\isido\\Zotero\\storage\\PAB85JUX\\Buehler e.a. - 2019 - Deep Hedging Hedging Derivatives Under Generic Ma.pdf:application/pdf},
}

@article{hutzenthaler_overcoming_2020,
	title = {Overcoming the curse of dimensionality in the numerical approximation of semilinear parabolic partial differential equations},
	volume = {476},
	issn = {1364-5021, 1471-2946},
	url = {http://arxiv.org/abs/1807.01212},
	doi = {10.1098/rspa.2019.0630},
	abstract = {For a long time it is well-known that high-dimensional linear parabolic partial differential equations (PDEs) can be approximated by Monte Carlo methods with a computational effort which grows polynomially both in the dimension and in the reciprocal of the prescribed accuracy. In other words, linear PDEs do not suffer from the curse of dimensionality. For general semilinear PDEs with Lipschitz coefficients, however, it remained an open question whether these suffer from the curse of dimensionality. In this paper we partially solve this open problem. More precisely, we prove in the case of semilinear heat equations with gradient-independent and globally Lipschitz continuous nonlinearities that the computational effort of a variant of the recently introduced multilevel Picard approximations grows polynomially both in the dimension and in the reciprocal of the required accuracy.},
	number = {2244},
	urldate = {2024-04-04},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas and Nguyen, Tuan Anh and von Wurstemberger, Philippe},
	month = dec,
	year = {2020},
	note = {arXiv:1807.01212 [cs, math]},
	keywords = {monte carlo, PDE},
	pages = {20190630},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\7BA94XXE\\1807.html:text/html;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\M52TXUY8\\Hutzenthaler e.a. - 2020 - Overcoming the curse of dimensionality in the nume.pdf:application/pdf},
}

@article{henry-labordere_deep_2017,
	title = {Deep {Primal}-{Dual} {Algorithm} for {BSDEs}: {Applications} of {Machine} {Learning} to {CVA} and {IM}},
	issn = {1556-5068},
	shorttitle = {Deep {Primal}-{Dual} {Algorithm} for {BSDEs}},
	url = {https://www.ssrn.com/abstract=3071506},
	doi = {10.2139/ssrn.3071506},
	abstract = {Building heavily on the recent nice paper [18], we introduce a primal-dual method for solving BSDEs based on the use of neural networks, stochastic gradient descent and a dual formulation of stochastic control problems [9]. Our algorithm is illustrated with two examples relevant in Mathematical Finance: the pricing of counterparty risk and the computation of initial margin.},
	language = {en},
	urldate = {2024-04-04},
	journal = {SSRN Electronic Journal},
	author = {Henry-Labordere, Pierre},
	year = {2017},
	keywords = {deep learning, bsde},
	file = {Henry-Labordere - 2017 - Deep Primal-Dual Algorithm for BSDEs Applications.pdf:C\:\\Users\\isido\\Zotero\\storage\\9VZBVEVH\\Henry-Labordere - 2017 - Deep Primal-Dual Algorithm for BSDEs Applications.pdf:application/pdf},
}

@article{rogers_monte_2002,
	title = {Monte {Carlo} valuation of {American} options},
	volume = {12},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {0960-1627, 1467-9965},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1467-9965.02010},
	doi = {10.1111/1467-9965.02010},
	abstract = {This paper introduces a dual way to price American options, based on simulating the paths of the option payoﬀ, and of a judiciously chosen Lagrangian martingale. Taking the pathwise maximum of the payoﬀ less the martingale provides an upper bound for the price of the option, and this bound is sharp for the optimal choice of Lagrangian martingale. As a ﬁrst exploration of this method, four examples are investigated numerically; the accuracy achieved with even very simple choices of Lagrangian martingale is surprising. The method also leads naturally to candidate hedging policies for the option, and estimates of the risk involved in using them.},
	language = {en},
	number = {3},
	urldate = {2024-04-04},
	journal = {Mathematical Finance},
	author = {Rogers, L. C. G.},
	month = jul,
	year = {2002},
	keywords = {monte carlo, finance, american option},
	pages = {271--286},
	file = {Rogers - 2002 - Monte Carlo valuation of American options.pdf:C\:\\Users\\isido\\Zotero\\storage\\3EUVSBWK\\Rogers - 2002 - Monte Carlo valuation of American options.pdf:application/pdf},
}

@misc{raissi_forward-backward_2018,
	title = {Forward-{Backward} {Stochastic} {Neural} {Networks}: {Deep} {Learning} of {High}-dimensional {Partial} {Differential} {Equations}},
	shorttitle = {Forward-{Backward} {Stochastic} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1804.07010},
	abstract = {Classical numerical methods for solving partial differential equations suffer from the curse dimensionality mainly due to their reliance on meticulously generated spatio-temporal grids. Inspired by modern deep learning based techniques for solving forward and inverse problems associated with partial differential equations, we circumvent the tyranny of numerical discretization by devising an algorithm that is scalable to high-dimensions. In particular, we approximate the unknown solution by a deep neural network which essentially enables us to benefit from the merits of automatic differentiation. To train the aforementioned neural network we leverage the well-known connection between high-dimensional partial differential equations and forward-backward stochastic differential equations. In fact, independent realizations of a standard Brownian motion will act as training data. We test the effectiveness of our approach for a couple of benchmark problems spanning a number of scientific domains including Black-Scholes-Barenblatt and Hamilton-Jacobi-Bellman equations, both in 100-dimensions.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Raissi, Maziar},
	month = apr,
	year = {2018},
	note = {arXiv:1804.07010 [cs, math, stat]},
	keywords = {PDE, deep learning, bsde},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\HWUW6MY4\\1804.html:text/html;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\2SNDTGW2\\Raissi - 2018 - Forward-Backward Stochastic Neural Networks Deep .pdf:application/pdf},
}

@article{jiang_convergence_2021,
	title = {Convergence of the {Deep} {BSDE} method for {FBSDEs} with non-{Lipschitz} coefficients},
	volume = {6},
	issn = {2095-9672, 2367-0126},
	url = {http://arxiv.org/abs/2101.01869},
	doi = {10.3934/puqr.2021019},
	abstract = {This paper is dedicated to solving high-dimensional coupled FBSDEs with non-Lipschitz diffusion coefficients numerically. Under mild conditions, we provided a posterior estimate of the numerical solution that holds for any time duration. This posterior estimate validates the convergence of the recently proposed Deep BSDE method. In addition, we developed a numerical scheme based on the Deep BSDE method and presented numerical examples in financial markets to demonstrate the high performance.},
	number = {4},
	urldate = {2024-04-06},
	journal = {Probability, Uncertainty and Quantitative Risk},
	author = {Jiang, Yifan and Li, Jinfeng},
	year = {2021},
	note = {arXiv:2101.01869 [cs, math]},
	keywords = {bsde},
	pages = {391},
	annote = {Comment: 19 pages, 2 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\CN2GIDE7\\2101.html:text/html;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\ID92WC9A\\Jiang en Li - 2021 - Convergence of the Deep BSDE method for FBSDEs wit.pdf:application/pdf},
}

@article{glau_deep_2022,
	title = {The deep parametric {PDE} method and applications to option pricing},
	volume = {432},
	issn = {00963003},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0096300322004295},
	doi = {10.1016/j.amc.2022.127355},
	language = {en},
	urldate = {2024-04-06},
	journal = {Applied Mathematics and Computation},
	author = {Glau, Kathrin and Wunderlich, Linus},
	month = nov,
	year = {2022},
	keywords = {finance, deep learning},
	pages = {127355},
	file = {Glau en Wunderlich - 2022 - The deep parametric PDE method and applications to.pdf:C\:\\Users\\isido\\Zotero\\storage\\37XNY24B\\Glau en Wunderlich - 2022 - The deep parametric PDE method and applications to.pdf:application/pdf},
}

@article{derivatives_nonlinear_nodate,
	title = {Nonlinear {Option} {Pricing}},
	language = {en},
	author = {Derivatives, American-Style and Detemple, Jerome},
	keywords = {finance, bsde},
	file = {Derivatives en Detemple - Nonlinear Option Pricing.pdf:C\:\\Users\\isido\\Zotero\\storage\\W2PLGWVL\\Derivatives en Detemple - Nonlinear Option Pricing.pdf:application/pdf},
}

@misc{assabumrungrat_error_2023,
	title = {Error {Analysis} of {Option} {Pricing} via {Deep} {PDE} {Solvers}: {Empirical} {Study}},
	shorttitle = {Error {Analysis} of {Option} {Pricing} via {Deep} {PDE} {Solvers}},
	url = {http://arxiv.org/abs/2311.07231},
	abstract = {Option pricing, a fundamental problem in finance, often requires solving non-linear partial differential equations (PDEs). When dealing with multi-asset options, such as rainbow options, these PDEs become high-dimensional, leading to challenges posed by the curse of dimensionality. While deep learning-based PDE solvers have recently emerged as scalable solutions to this high-dimensional problem, their empirical and quantitative accuracy remains not well-understood, hindering their real-world applicability. In this study, we aimed to offer actionable insights into the utility of Deep PDE solvers for practical option pricing implementation. Through comparative experiments, we assessed the empirical performance of these solvers in high-dimensional contexts. Our investigation identified three primary sources of errors in Deep PDE solvers: (i) errors inherent in the specifications of the target option and underlying assets, (ii) errors originating from the asset model simulation methods, and (iii) errors stemming from the neural network training. Through ablation studies, we evaluated the individual impact of each error source. Our results indicate that the Deep BSDE method (DBSDE) is superior in performance and exhibits robustness against variations in option specifications. In contrast, some other methods are overly sensitive to option specifications, such as time to expiration. We also find that the performance of these methods improves inversely proportional to the square root of batch size and the number of time steps. This observation can aid in estimating computational resources for achieving desired accuracies with Deep PDE solvers.},
	urldate = {2024-04-06},
	publisher = {arXiv},
	author = {Assabumrungrat, Rawin and Minami, Kentaro and Hirano, Masanori},
	month = nov,
	year = {2023},
	note = {arXiv:2311.07231 [cs, math, q-fin]},
	keywords = {deep learning},
	annote = {Comment: 11 pages, 6 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\V8HA6FXJ\\2311.html:text/html;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\WWMAJGWG\\Assabumrungrat e.a. - 2023 - Error Analysis of Option Pricing via Deep PDE Solv.pdf:application/pdf},
}

@misc{chen_deep_2019,
	title = {Deep {Neural} {Network} {Framework} {Based} on {Backward} {Stochastic} {Differential} {Equations} for {Pricing} and {Hedging} {American} {Options} in {High} {Dimensions}},
	url = {http://arxiv.org/abs/1909.11532},
	abstract = {We propose a deep neural network framework for computing prices and deltas of American options in high dimensions. The architecture of the framework is a sequence of neural networks, where each network learns the difference of the price functions between adjacent timesteps. We introduce the least squares residual of the associated backward stochastic differential equation as the loss function. Our proposed framework yields prices and deltas on the entire spacetime, not only at a given point. The computational cost of the proposed approach is quadratic in dimension, which addresses the curse of dimensionality issue that state-of-the-art approaches suffer. Our numerical simulations demonstrate these contributions, and show that the proposed neural network framework outperforms state-of-the-art approaches in high dimensions.},
	urldate = {2024-04-06},
	publisher = {arXiv},
	author = {Chen, Yangang and Wan, Justin W. L.},
	month = sep,
	year = {2019},
	note = {arXiv:1909.11532 [cs, q-fin, stat]},
	keywords = {finance, deep learning, bsde},
	annote = {Comment: 35 pages, 11 figures, 15 tables},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\IFYUNVDJ\\1909.html:text/html;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\P49LVVAB\\Chen en Wan - 2019 - Deep Neural Network Framework Based on Backward St.pdf:application/pdf},
}

@misc{hure_deep_2020,
	title = {Deep backward schemes for high-dimensional nonlinear {PDEs}},
	url = {http://arxiv.org/abs/1902.01599},
	abstract = {We propose new machine learning schemes for solving high dimensional nonlinear partial differential equations (PDEs). Relying on the classical backward stochastic differential equation (BSDE) representation of PDEs, our algorithms estimate simultaneously the solution and its gradient by deep neural networks. These approximations are performed at each time step from the minimization of loss functions defined recursively by backward induction. The methodology is extended to variational inequalities arising in optimal stopping problems. We analyze the convergence of the deep learning schemes and provide error estimates in terms of the universal approximation of neural networks. Numerical results show that our algorithms give very good results till dimension 50 (and certainly above), for both PDEs and variational inequalities problems. For the PDEs resolution, our results are very similar to those obtained by the recent method in {\textbackslash}cite\{weinan2017deep\} when the latter converges to the right solution or does not diverge. Numerical tests indicate that the proposed methods are not stuck in poor local minimaas it can be the case with the algorithm designed in {\textbackslash}cite\{weinan2017deep\}, and no divergence is experienced. The only limitation seems to be due to the inability of the considered deep neural networks to represent a solution with a too complex structure in high dimension.},
	urldate = {2024-04-06},
	publisher = {arXiv},
	author = {Huré, Côme and Pham, Huyên and Warin, Xavier},
	month = jun,
	year = {2020},
	note = {arXiv:1902.01599 [cs, math, stat]},
	keywords = {PDE, deep learning},
	annote = {Comment: 34 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\23AHB6QC\\1902.html:text/html;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\CYZBAHZ4\\Huré e.a. - 2020 - Deep backward schemes for high-dimensional nonline.pdf:application/pdf},
}

@misc{fujii_asymptotic_2019,
	title = {Asymptotic {Expansion} as {Prior} {Knowledge} in {Deep} {Learning} {Method} for high dimensional {BSDEs}},
	url = {http://arxiv.org/abs/1710.07030},
	abstract = {We demonstrate that the use of asymptotic expansion as prior knowledge in the "deep BSDE solver", which is a deep learning method for high dimensional BSDEs proposed by Weinan E, Han \& Jentzen (2017), drastically reduces the loss function and accelerates the speed of convergence. We illustrate the technique and its implications by using Bergman's model with different lending and borrowing rates as a typical model for FVA as well as a class of solvable BSDEs with quadratic growth drivers. We also present an extension of the deep BSDE solver for reflected BSDEs representing American option prices.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Fujii, Masaaki and Takahashi, Akihiko and Takahashi, Masayuki},
	month = mar,
	year = {2019},
	note = {arXiv:1710.07030 [q-fin]},
	keywords = {deep learning, bsde},
	annote = {Comment: Forthcoming in APFM},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\2MJ7BW8Q\\1710.html:text/html;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\HYU366T2\\Fujii e.a. - 2019 - Asymptotic Expansion as Prior Knowledge in Deep Le.pdf:application/pdf},
}

@article{liang_deep_2021,
	title = {Deep learning-based least squares forward-backward stochastic differential equation solver for high-dimensional derivative pricing},
	volume = {21},
	issn = {1469-7688, 1469-7696},
	url = {https://www.tandfonline.com/doi/full/10.1080/14697688.2021.1881149},
	doi = {10.1080/14697688.2021.1881149},
	abstract = {We propose a new forward-backward stochastic differential equation solver for high-dimensional derivative pricing problems by combining a deep learning solver with a least squares regression technique widely used in the least squares Monte Carlo method for the valuation of American options. Our numerical experiments demonstrate the accuracy of our least squares backward deep neural network solver and its capability to produce accurate prices for complex early exercisable derivatives, such as callable yield notes. Our method can serve as a generic numerical solver for pricing derivatives across various asset groups, in particular, as an accurate means for pricing high-dimensional derivatives with early exercise features.},
	language = {en},
	number = {8},
	urldate = {2024-04-09},
	journal = {Quantitative Finance},
	author = {Liang, Jian and Xu, Zhe and Li, Peter},
	month = aug,
	year = {2021},
	keywords = {finance, deep learning, bsde},
	pages = {1309--1323},
	file = {Liang e.a. - 2021 - Deep learning-based least squares forward-backward.pdf:C\:\\Users\\isido\\Zotero\\storage\\IMQ7W4IR\\Liang e.a. - 2021 - Deep learning-based least squares forward-backward.pdf:application/pdf},
}

@article{na_efficient_2023,
	title = {Efficient {Pricing} and {Hedging} of {High} {Dimensional} {American} {Options} {Using} {Recurrent} {Networks}},
	volume = {23},
	issn = {1469-7688, 1469-7696},
	url = {http://arxiv.org/abs/2301.08232},
	doi = {10.1080/14697688.2023.2167666},
	abstract = {We propose a deep Recurrent neural network (RNN) framework for computing prices and deltas of American options in high dimensions. Our proposed framework uses two deep RNNs, where one network learns the price and the other learns the delta of the option for each timestep. Our proposed framework yields prices and deltas for the entire spacetime, not only at a given point (e.g. t = 0). The computational cost of the proposed approach is linear in time, which improves on the quadratic time seen for feedforward networks that price American options. The computational memory cost of our method is constant in memory, which is an improvement over the linear memory costs seen in feedforward networks. Our numerical simulations demonstrate these contributions, and show that the proposed deep RNN framework is computationally more efficient than traditional feedforward neural network frameworks in time and memory.},
	number = {4},
	urldate = {2024-04-09},
	journal = {Quantitative Finance},
	author = {Na, Andrew and Wan, Justin},
	month = apr,
	year = {2023},
	note = {arXiv:2301.08232 [cs, q-fin]},
	keywords = {finance, deep learning, american option},
	pages = {631--651},
	file = {arXiv.org Snapshot:C\:\\Users\\isido\\Zotero\\storage\\EAT2PC49\\2301.html:text/html;Full Text PDF:C\:\\Users\\isido\\Zotero\\storage\\3DVRHA48\\Na en Wan - 2023 - Efficient Pricing and Hedging of High Dimensional .pdf:application/pdf},
}

@article{haugh_pricing_nodate,
	title = {Pricing {American} {Options}: {A} {Duality} {Approach}},
	abstract = {We develop a new method for pricing American options. The main practical contribution of this paper is a general algorithm for constructing upper and lower bounds on the true price of the option using any approximation to the option price. We show that our bounds are tight, so that if the initial approximation is close to the true price of the option, the bounds are also guaranteed to be close. We also explicitly characterize the worst-case performance of the pricing bounds. The computation of the lower bound is straightforward and relies on simulating the suboptimal exercise strategy implied by the approximate option price. The upper bound is also computed using Monte Carlo simulation. This is made feasible by the representation of the American option price as a solution of a properly deﬁned dual minimization problem, which is the main theoretical result of this paper. Our algorithm proves to be accurate on a set of sample problems where we price call options on the maximum and the geometric mean of a collection of stocks. These numerical results suggest that our pricing method can be successfully applied to problems of practical interest.},
	language = {en},
	author = {Haugh, Martin B and Kogan, Leonid},
	file = {Haugh en Kogan - Pricing American Options A Duality Approach.pdf:C\:\\Users\\isido\\Zotero\\storage\\9XGC5NE7\\Haugh en Kogan - Pricing American Options A Duality Approach.pdf:application/pdf},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	annote = {Comment: Tech report},
	file = {He e.a. - 2015 - Deep Residual Learning for Image Recognition.pdf:C\:\\Users\\isido\\Zotero\\storage\\MLFYSTUJ\\He e.a. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@misc{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classiﬁcation model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a signiﬁcant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classiﬁcation: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	language = {en},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv:1502.03167 [cs]},
	file = {Ioffe en Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:C\:\\Users\\isido\\Zotero\\storage\\HN3SNX9E\\Ioffe en Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf},
}

@misc{pfeiffer_stochastic_2018,
	title = {On stochastic optimization methods for {Monte} {Carlo} least-squares problems},
	url = {http://arxiv.org/abs/1804.10079},
	abstract = {This work presents stochastic optimization methods targeted at least-squares problems involving Monte Carlo integration. While the most common approach to solving these problems is to apply stochastic gradient descent (SGD) or similar methods such as AdaGrad [5] and Adam [12], which involve estimating a stochastic gradient from a small number of Monte Carlo samples computed at each iteration, we show that for this category of problems it is possible to achieve faster asymptotic convergence rates using an increasing number of samples per iteration instead, a strategy we call increasing precision (IP). We then improve pre-asymptotic convergence by introducing a hybrid approach that combines the qualities of increasing precision and otherwise “constant” precision, resulting in methods such as the IP-SGD hybrid and IP-AdaGrad hybrid, essentially by modifying their gradient estimators to have an equivalent eﬀect to increasing precision. Finally, we observe that, in some problems, incorporating a Gauss-Newton preconditioner to the IP-SGD hybrid method can provide much better convergence than employing a Quasi-Newton approach or covariance-preconditioning as in AdaGrad or Adam.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Pfeiffer, Gustavo T. and Sato, Yoichi},
	month = apr,
	year = {2018},
	note = {arXiv:1804.10079 [math]},
	file = {Pfeiffer en Sato - 2018 - On stochastic optimization methods for Monte Carlo.pdf:C\:\\Users\\isido\\Zotero\\storage\\I99TZJHY\\Pfeiffer en Sato - 2018 - On stochastic optimization methods for Monte Carlo.pdf:application/pdf},
}

@article{chen_deep_2021,
	title = {Deep neural network framework based on backward stochastic differential equations for pricing and hedging {American} options in high dimensions},
	volume = {21},
	issn = {1469-7688, 1469-7696},
	url = {https://www.tandfonline.com/doi/full/10.1080/14697688.2020.1788219},
	doi = {10.1080/14697688.2020.1788219},
	language = {en},
	number = {1},
	urldate = {2024-06-12},
	journal = {Quantitative Finance},
	author = {Chen, Yangang and Wan, Justin W. L.},
	month = jan,
	year = {2021},
	pages = {45--67},
	file = {Chen en Wan - 2021 - Deep neural network framework based on backward st.pdf:C\:\\Users\\isido\\Zotero\\storage\\XTC4EZ68\\Chen en Wan - 2021 - Deep neural network framework based on backward st.pdf:application/pdf},
}
